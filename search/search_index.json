{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to statistics with R Teachers Rachel Marcone Joao Lourenco Material Google doc (through mail) General learning outcomes At the end of this course, participants will be able to: - have experience in the application of basic statistics techniques - know how to summarize data with numerical and graphical summaries - plot data - do hypothesis testing and multiple testing correction - linear models - correlation and regression - principal component analysis and other topics Learning outcomes explained To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn. Learning experiences The course will combine lectures on statistics and practical exercises, during which the participants will learn how to work with the widely used \u201cR\u201d language and environment for statistical computing and graphics. Participants will also have the opportunity to ask questions about the analysis of their own data. Exercises Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. Some answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different. Asking questions During lectures, you are encouraged to raise your hand if you have questions.","title":"Home"},{"location":"#introduction-to-statistics-with-r","text":"","title":"Introduction to statistics with R"},{"location":"#teachers","text":"Rachel Marcone Joao Lourenco","title":"Teachers"},{"location":"#material","text":"Google doc (through mail)","title":"Material"},{"location":"#general-learning-outcomes","text":"At the end of this course, participants will be able to: - have experience in the application of basic statistics techniques - know how to summarize data with numerical and graphical summaries - plot data - do hypothesis testing and multiple testing correction - linear models - correlation and regression - principal component analysis and other topics","title":"General learning outcomes"},{"location":"#learning-outcomes-explained","text":"To reach the general learning outcomes above, we have set a number of smaller learning outcomes. Each chapter starts with these smaller learning outcomes. Use these at the start of a chapter to get an idea what you will learn. Use them also at the end of a chapter to evaluate whether you have learned what you were expected to learn.","title":"Learning outcomes explained"},{"location":"#learning-experiences","text":"The course will combine lectures on statistics and practical exercises, during which the participants will learn how to work with the widely used \u201cR\u201d language and environment for statistical computing and graphics. Participants will also have the opportunity to ask questions about the analysis of their own data.","title":"Learning experiences"},{"location":"#exercises","text":"Each block has practical work involved. Some more than others. The practicals are subdivided into chapters, and we\u2019ll have a (short) discussion after each chapter. Some answers to the practicals are incorporated, but they are hidden. Do the exercise first by yourself, before checking out the answer. If your answer is different from the answer in the practicals, try to figure out why they are different.","title":"Exercises"},{"location":"#asking-questions","text":"During lectures, you are encouraged to raise your hand if you have questions.","title":"Asking questions"},{"location":"bonus_code/","text":"Bonus code","title":"Bonus code"},{"location":"bonus_code/#bonus-code","text":"","title":"Bonus code"},{"location":"course_schedule/","text":"Note Apart from the starting time the time schedule is indicative . Because we can not plan a course by the minute, in practice the time points will deviate. Day 1 block start end subject introduction 9:15 AM 9:30 AM Simple and multiple linear regression block 1 9:30 AM 10:30 AM 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Day 2 block start end subject block 1 9:00 AM 10:30 AM Generalized linear models 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Further exercises 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Further exercises Day 3 block start end subject block 1 9:00 AM 10:30 AM Mixed-effects linear models 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Longitudinal data analysis 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Day 4 block start end subject block 1 9:00 AM 10:30 AM Generalized additive models 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Generate markdown tables at tablesgenerator.com","title":"Course schedule"},{"location":"course_schedule/#day-1","text":"block start end subject introduction 9:15 AM 9:30 AM Simple and multiple linear regression block 1 9:30 AM 10:30 AM 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM","title":"Day 1"},{"location":"course_schedule/#day-2","text":"block start end subject block 1 9:00 AM 10:30 AM Generalized linear models 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM Further exercises 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Further exercises","title":"Day 2"},{"location":"course_schedule/#day-3","text":"block start end subject block 1 9:00 AM 10:30 AM Mixed-effects linear models 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM Longitudinal data analysis 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM","title":"Day 3"},{"location":"course_schedule/#day-4","text":"block start end subject block 1 9:00 AM 10:30 AM Generalized additive models 10:30 AM 11:00 AM BREAK block 2 11:00 AM 12:30 PM 12:30 PM 1:30 PM BREAK block 3 1:30 PM 3:00 PM 3:00 PM 3:30 PM BREAK block 4 3:30 PM 5:00 PM Generate markdown tables at tablesgenerator.com","title":"Day 4"},{"location":"day1/","text":"Descriptive Statistics and Exploratory Data Analysis In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. Slides of lectures: Download slides Morning Lecture Download slides Morning Lecture 2 Download slides Afternoon Lecture Data for exercises: Download full data for the week The purpose of this exercise is to introduce you to using R for statistical analysis of data. Code that you can copy and paste is provided here to get you started. You will get the most out of the session if you also do some exploring on your own: check the help files for each function to learn what default values and optional arguments are there, and try out your own variations. Preliminaries: Getting help in R An excellent source of R documentation is the Comprehensive R Archive Network, or CRAN. There is a Swiss mirror site at http://cran.ch.r-project.org/. If you go to that site, you will find several links under Documentation (the fourth major entry on the left side). \u201cOfficial\u201d documentation is available under Manuals; other helpful documentation is under Contributed. For additional practice, you can also download R and add-on packages onto your own computer at home if you have one. To proceed, you will need to start R. In Windows there should either be a desktop shortcut or you should find it in the Start menu; in Linux, just type R at the prompt. You should become acquainted with the help facility within R, it can be your friend! The basic help command is help () within the parentheses you would type (inside of double quotes) the name of a function whose help file you want to see, e.g. help ( \"mean\" ) You can also use the alternative syntax ? mean If you don\u2019t know the exact command name, use help.search () with the name of the concept inside double quotes within the parentheses. Getting Data into R R has a number of functions to create data vectors, including: c(), seq(), rep(). Find out what each of these do, and make some data vectors of your choice using each. To get some practice using statistical functions and performing small calculations in R, create a weight and corresponding height vector for computing body mass index (bmi) (this example is inspired by Dalgaard\u2019s book, Introductory statistics with R): weight <- c ( 65 , 72 , 55 , 91 , 95 , 72 ) height <- c ( 1.73 , 1.80 , 1.62 , 1.90 , 1.78 , 1.93 ) bmi <- weight / height ^ 2 bmi # Type this in R to see the computed values The # sign indicates a comment: anything occurring after this sign on a line is ignored by R (but can be very useful in programming at it provides a means for documenting your code). Hint Practice this throughout the course! These data vectors are a little too small to really require summaries. It is a little more interesting to look at real data. Hellung Data We are going to load the package ISwR, and examine the variables in the data set hellung. First, we need to make sure the package is installed. From R Studio, you can go to the menu Tools -> Install packages\u2026, and then choose the package you need installed. Using the RGui under Windows, you can go to menu Packages -> Install package(s) In the console, you can use the install.packages command: install.packages(\u201cISwR\u201d). R packages have an explanation on installation, which you can find in each help manual of the package. Once installed you can load the library as well as the data hellung. library ( ISwR ) ? hellung data ( hellung ) Univariate numerical summaries You can find the variable names with names ( hellung ) \u2026 and can summarize the data set with summary ( hellung ) All good ? Also compute the mean and sd for each variable. Which of the variables does it not make sense to summarize like this? Univariate graphical summaries Make histograms of each of the variables. par ( mfrow = c ( 2 , 2 )) # for viewing multiple plots (2 rows x 2 columns = 4 plots) hist ( hellung $ conc ) hist ( hellung $ diameter ) hist ( hellung $ glucose ) Make a boxplot of the variable conc. Now, make side by side boxplots of conc, one for each value of glucose; do the same with diameter. Note: the conc ~ glucose notation means \u201cexplain conc according to glucose\u201d; it tells R that it should split the boxplot according to the different values of the \u201cglucose\u201d variable. par ( mfrow = c ( 2 , 2 )) # for viewing multiple plots boxplot ( conc ~ glucose , data = hellung ) boxplot ( diameter ~ glucose , data = hellung ) Does the distribution (pattern of variability) of either variable appear to depend on the presence or absence of glucose? Do we have enough information to decide whether glucose is causing any difference? A bivariate look It is also interesting to further explore relationships between different variables. We have already looked informally at the relationship between glucose and the other variables. We can also explore the relationship between the numerical variables conc and diameter: cor ( hellung $ conc , hellung $ diameter ) plot ( diameter ~ conc , data = hellung ) Do you see any structure in the scatterplot? What happens if we take log(conc) instead of conc? Importing and exporting data into R Usually, the data to be analysed in R is already available in another program, typically Excel, and must be imported into R. You can read many different file formats in R, including text files and Excel files. However, since Excel files can be complex (including, for example, merged cells that are hard to understand), it is recommended in most cases to export them to text format first, either \u201cCSV\u201d (Comma-separated variables) or \u201ctab delimited\u201d, and to make sure that the result is correct, before loading them into R. Typical R commands for reading these files are read.table, read.delim, read.csv. The help pages can tell you the differences between these commands, but read.csv is the one to use for CSV files. One important caveat is the configuration of your computer with regards to the decimal point: if Excel saves files using commas for the decimal separator (e.g. 10,00 instead of 10.00), R will not recognize the data as numbers because of the \u201cparasite\u201d character. The option dec = \u201c,\u201d can be used if necessary to modify this behaviour. Conversely, the write.table command can be used to write a table to a file for subsequent reading into Excel. When using R studio, you can use the \u201cimport dataset\u201d tool, that will allow you to explore the structure of the data you import. A useful feature of this tool is that, when finished, it will not only load the data, but will also print the actual R command that was used to do so, allowing you to copy it to your script for future use. Note: recent versions of R Studio load data into a variable that is not a data frame, but a more advanced structure. The resulting variable works mostly like a data frame, but there are some differences. If you have any issue, try converting it back to a data frame. For example, if you loaded data using the importer tool, you can convert it to a data frame using data2 <- as.data.frame(data) Looking at some unknown data The data for this exercice is provided in an Excel file, data.xls. You need to export this files from Excel to either CSV or text (tab-delimited) files, and then read it in R using one of the following commands: data <- read.table ( \"data.txt\" , header = TRUE ) # Reads a tab-delimited file and tells R that # the first line actually contains a header data <- read.csv ( \"data.csv\" ) # Reads a CSV file The file contains three datasets in three columns of the file. Start by looking at some summaries of the data: data summary ( data ) sd ( data [, 1 ]); sd ( data [, 2 ]); sd ( data [, 3 ]) What comment can you make about these datasets ? The individual datasets can be accessed by using one of the (equivalent) commands data $ data1 # Column named \"data1\" data [, 1 ] # First column (= column \"data1\") It may be easier to copy them in separate variables: data1 <- data $ data1 summary ( data1 ) or, equivalently: attach ( data ) summary ( data1 ) While these numbers are interesting, they are only a very short summary of the data, as you know by now. We are going to plot the data in several different ways. Firstly, let us plot the usual barplot with standard deviation; is it very informative ? means <- as.vector ( colMeans ( data )) # means for the 3 datasets sds <- as.vector ( sapply ( data , sd )) # SDs for the 3 datasets # bp will contain the x coordinates of the three barplots # ylim is used to make sure that some space is left for the error bar bp <- barplot ( means , ylim = 1.1 * range ( 0 , means + sds ), names.arg = c ( \"Data1\" , \"Data2\" , \"Data3\" )) arrows ( as.vector ( bp ), means , as.vector ( bp ), means + sds , angle = 90 , code = 3 ) Let us look at 4 different ways of plotting the data. In the case of the histogram, you can change the number of bars if necessary by adding the argument breaks=n. datatoplot <- data [, 1 ] Plot 4 rows of graphs on one plot par ( mfrow = c ( 4 , 1 )) 1st plot: individual points on the x-axis; random noise on the y-axis so that points are not too much superimposed plot ( datatoplot , runif ( length ( datatoplot ), -1 , 1 ), xlim = range ( datatoplot )) 2nd plot: histogram, with the density line superimposed hist ( datatoplot , freq = F , xlim = range ( datatoplot )) lines ( density ( datatoplot )) 3rd plot: average +/- Sd plot ( mean ( datatoplot ), 0 , xlim = range ( datatoplot ), main = \"Mean and standard deviation of a\" ) arrows ( mean ( datatoplot ) - sd ( datatoplot ), 0 , mean ( datatoplot ) + sd ( datatoplot ), 0 , angle = 90 , code = 3 ) 4th plot: boxplot boxplot ( datatoplot , horizontal = TRUE , ylim = range ( datatoplot )) Do these plots for the three different datasets. Are there cases where some plots are more adapted to the data than others ? What about the number of bars in the histograms ? You will not need to save any R objects that you created today (unless you wish to), so feel free to \u2018clean up\u2019 after yourself with rm(). To remove all objects in your workspace (permanently and irreversibly, so be careful), type rm(list=ls()), or simply answer n when asked if you wish to save your workspace image. This question appears on the screen when you quit R; to quit, type q () Hint Before quitting, try just typing q without any parentheses. This might help you to remember that you need the parentheses! Looking at students data Load the file students.csv into R. It contains data collected from students at the Univerity of Lausanne. Look at the variables; try to know/explore the data: summarize the different variables numerically and graphically, and see if you can find relationships between them.","title":"Day 1"},{"location":"day1/#descriptive-statistics-and-exploratory-data-analysis","text":"In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. Slides of lectures: Download slides Morning Lecture Download slides Morning Lecture 2 Download slides Afternoon Lecture Data for exercises: Download full data for the week The purpose of this exercise is to introduce you to using R for statistical analysis of data. Code that you can copy and paste is provided here to get you started. You will get the most out of the session if you also do some exploring on your own: check the help files for each function to learn what default values and optional arguments are there, and try out your own variations.","title":"Descriptive Statistics and Exploratory Data Analysis"},{"location":"day1/#preliminaries-getting-help-in-r","text":"An excellent source of R documentation is the Comprehensive R Archive Network, or CRAN. There is a Swiss mirror site at http://cran.ch.r-project.org/. If you go to that site, you will find several links under Documentation (the fourth major entry on the left side). \u201cOfficial\u201d documentation is available under Manuals; other helpful documentation is under Contributed. For additional practice, you can also download R and add-on packages onto your own computer at home if you have one. To proceed, you will need to start R. In Windows there should either be a desktop shortcut or you should find it in the Start menu; in Linux, just type R at the prompt. You should become acquainted with the help facility within R, it can be your friend! The basic help command is help () within the parentheses you would type (inside of double quotes) the name of a function whose help file you want to see, e.g. help ( \"mean\" ) You can also use the alternative syntax ? mean If you don\u2019t know the exact command name, use help.search () with the name of the concept inside double quotes within the parentheses.","title":"Preliminaries: Getting help in R"},{"location":"day1/#getting-data-into-r","text":"R has a number of functions to create data vectors, including: c(), seq(), rep(). Find out what each of these do, and make some data vectors of your choice using each. To get some practice using statistical functions and performing small calculations in R, create a weight and corresponding height vector for computing body mass index (bmi) (this example is inspired by Dalgaard\u2019s book, Introductory statistics with R): weight <- c ( 65 , 72 , 55 , 91 , 95 , 72 ) height <- c ( 1.73 , 1.80 , 1.62 , 1.90 , 1.78 , 1.93 ) bmi <- weight / height ^ 2 bmi # Type this in R to see the computed values The # sign indicates a comment: anything occurring after this sign on a line is ignored by R (but can be very useful in programming at it provides a means for documenting your code). Hint Practice this throughout the course! These data vectors are a little too small to really require summaries. It is a little more interesting to look at real data.","title":"Getting Data into R"},{"location":"day1/#hellung-data","text":"We are going to load the package ISwR, and examine the variables in the data set hellung. First, we need to make sure the package is installed. From R Studio, you can go to the menu Tools -> Install packages\u2026, and then choose the package you need installed. Using the RGui under Windows, you can go to menu Packages -> Install package(s) In the console, you can use the install.packages command: install.packages(\u201cISwR\u201d). R packages have an explanation on installation, which you can find in each help manual of the package. Once installed you can load the library as well as the data hellung. library ( ISwR ) ? hellung data ( hellung )","title":"Hellung Data"},{"location":"day1/#univariate-numerical-summaries","text":"You can find the variable names with names ( hellung ) \u2026 and can summarize the data set with summary ( hellung ) All good ? Also compute the mean and sd for each variable. Which of the variables does it not make sense to summarize like this?","title":"Univariate numerical summaries"},{"location":"day1/#univariate-graphical-summaries","text":"Make histograms of each of the variables. par ( mfrow = c ( 2 , 2 )) # for viewing multiple plots (2 rows x 2 columns = 4 plots) hist ( hellung $ conc ) hist ( hellung $ diameter ) hist ( hellung $ glucose ) Make a boxplot of the variable conc. Now, make side by side boxplots of conc, one for each value of glucose; do the same with diameter. Note: the conc ~ glucose notation means \u201cexplain conc according to glucose\u201d; it tells R that it should split the boxplot according to the different values of the \u201cglucose\u201d variable. par ( mfrow = c ( 2 , 2 )) # for viewing multiple plots boxplot ( conc ~ glucose , data = hellung ) boxplot ( diameter ~ glucose , data = hellung ) Does the distribution (pattern of variability) of either variable appear to depend on the presence or absence of glucose? Do we have enough information to decide whether glucose is causing any difference?","title":"Univariate graphical summaries"},{"location":"day1/#a-bivariate-look","text":"It is also interesting to further explore relationships between different variables. We have already looked informally at the relationship between glucose and the other variables. We can also explore the relationship between the numerical variables conc and diameter: cor ( hellung $ conc , hellung $ diameter ) plot ( diameter ~ conc , data = hellung ) Do you see any structure in the scatterplot? What happens if we take log(conc) instead of conc?","title":"A bivariate look"},{"location":"day1/#importing-and-exporting-data-into-r","text":"Usually, the data to be analysed in R is already available in another program, typically Excel, and must be imported into R. You can read many different file formats in R, including text files and Excel files. However, since Excel files can be complex (including, for example, merged cells that are hard to understand), it is recommended in most cases to export them to text format first, either \u201cCSV\u201d (Comma-separated variables) or \u201ctab delimited\u201d, and to make sure that the result is correct, before loading them into R. Typical R commands for reading these files are read.table, read.delim, read.csv. The help pages can tell you the differences between these commands, but read.csv is the one to use for CSV files. One important caveat is the configuration of your computer with regards to the decimal point: if Excel saves files using commas for the decimal separator (e.g. 10,00 instead of 10.00), R will not recognize the data as numbers because of the \u201cparasite\u201d character. The option dec = \u201c,\u201d can be used if necessary to modify this behaviour. Conversely, the write.table command can be used to write a table to a file for subsequent reading into Excel. When using R studio, you can use the \u201cimport dataset\u201d tool, that will allow you to explore the structure of the data you import. A useful feature of this tool is that, when finished, it will not only load the data, but will also print the actual R command that was used to do so, allowing you to copy it to your script for future use. Note: recent versions of R Studio load data into a variable that is not a data frame, but a more advanced structure. The resulting variable works mostly like a data frame, but there are some differences. If you have any issue, try converting it back to a data frame. For example, if you loaded data using the importer tool, you can convert it to a data frame using data2 <- as.data.frame(data)","title":"Importing and exporting data into R"},{"location":"day1/#looking-at-some-unknown-data","text":"The data for this exercice is provided in an Excel file, data.xls. You need to export this files from Excel to either CSV or text (tab-delimited) files, and then read it in R using one of the following commands: data <- read.table ( \"data.txt\" , header = TRUE ) # Reads a tab-delimited file and tells R that # the first line actually contains a header data <- read.csv ( \"data.csv\" ) # Reads a CSV file The file contains three datasets in three columns of the file. Start by looking at some summaries of the data: data summary ( data ) sd ( data [, 1 ]); sd ( data [, 2 ]); sd ( data [, 3 ]) What comment can you make about these datasets ? The individual datasets can be accessed by using one of the (equivalent) commands data $ data1 # Column named \"data1\" data [, 1 ] # First column (= column \"data1\") It may be easier to copy them in separate variables: data1 <- data $ data1 summary ( data1 ) or, equivalently: attach ( data ) summary ( data1 ) While these numbers are interesting, they are only a very short summary of the data, as you know by now. We are going to plot the data in several different ways. Firstly, let us plot the usual barplot with standard deviation; is it very informative ? means <- as.vector ( colMeans ( data )) # means for the 3 datasets sds <- as.vector ( sapply ( data , sd )) # SDs for the 3 datasets # bp will contain the x coordinates of the three barplots # ylim is used to make sure that some space is left for the error bar bp <- barplot ( means , ylim = 1.1 * range ( 0 , means + sds ), names.arg = c ( \"Data1\" , \"Data2\" , \"Data3\" )) arrows ( as.vector ( bp ), means , as.vector ( bp ), means + sds , angle = 90 , code = 3 ) Let us look at 4 different ways of plotting the data. In the case of the histogram, you can change the number of bars if necessary by adding the argument breaks=n. datatoplot <- data [, 1 ]","title":"Looking at some unknown data"},{"location":"day1/#plot-4-rows-of-graphs-on-one-plot","text":"par ( mfrow = c ( 4 , 1 )) 1st plot: individual points on the x-axis; random noise on the y-axis so that points are not too much superimposed plot ( datatoplot , runif ( length ( datatoplot ), -1 , 1 ), xlim = range ( datatoplot )) 2nd plot: histogram, with the density line superimposed hist ( datatoplot , freq = F , xlim = range ( datatoplot )) lines ( density ( datatoplot )) 3rd plot: average +/- Sd plot ( mean ( datatoplot ), 0 , xlim = range ( datatoplot ), main = \"Mean and standard deviation of a\" ) arrows ( mean ( datatoplot ) - sd ( datatoplot ), 0 , mean ( datatoplot ) + sd ( datatoplot ), 0 , angle = 90 , code = 3 ) 4th plot: boxplot boxplot ( datatoplot , horizontal = TRUE , ylim = range ( datatoplot )) Do these plots for the three different datasets. Are there cases where some plots are more adapted to the data than others ? What about the number of bars in the histograms ? You will not need to save any R objects that you created today (unless you wish to), so feel free to \u2018clean up\u2019 after yourself with rm(). To remove all objects in your workspace (permanently and irreversibly, so be careful), type rm(list=ls()), or simply answer n when asked if you wish to save your workspace image. This question appears on the screen when you quit R; to quit, type q () Hint Before quitting, try just typing q without any parentheses. This might help you to remember that you need the parentheses!","title":"Plot 4 rows of graphs on one plot"},{"location":"day1/#looking-at-students-data","text":"Load the file students.csv into R. It contains data collected from students at the Univerity of Lausanne. Look at the variables; try to know/explore the data: summarize the different variables numerically and graphically, and see if you can find relationships between them.","title":"Looking at students data"},{"location":"day2/","text":"Hypothesis Testing In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. Slides of lectures: Download slides Morning Download slides Afternoon The purpose of this exercise is to help you to better interpret a p-value by using R for introducing you to some simple hypothesis testing functions. As usual, be sure to read the help documentation for any new functions. Exercise 1: One-sample t-test We use the intake data, available in the ISwR package. We will use the variable pre. library ( ISwR ) library ( ggplot2 ) #we will show you how to do some standard ggplots data ( intake ) ? intake attach ( intake ) intake Start off by looking at some simple summary statistics: mean, sd, quantile (hardly necessary for such a small data set, but good practice). Answer summary ( intake ) mean ( intake $ pre ) sd ( intake $ pre ) Might these data be approximately normally distributed? Answer # Assumption: no significant outliers in the data ggboxplot ( intake $ pre , width = 0.5 , add = c ( \"mean\" , \"jitter\" ), ylab = \"premenstrual intake\" , xlab = F ) identify_outliers ( as.data.frame ( intake $ pre )) # Assumption: normality ggqqplot ( intake , \"pre\" ) shapiro_test ( intake $ pre ) Suppose you wish to test whether there is a systematic deviation between the women\u2019s (pre) energy intake and a recommended value of 7725 kJ. Assuming that the data are from a normal distribution, we are interested in testing whether the (population) mean is 7725. We can do a t-test in R as follows: t.test ( pre , mu = 7725 ) Any idea what the argument alternative is doing ? t.test ( pre , mu = 7725 , alternative = \"less\" ) There are several components to the output, Take some time to make sure you can understand what it all means. For an alpha level of 0.05, do you reject the null hypothesis? What about for an alpha level of 0.01? The default assumes that you want a 2-sided test. Use help to find out how you could get a 1-sided test that would be meaningful for the dataset, and carry this out. For an alpha level of 0.01, do you reject the null hypothesis? Exercise 2: Two-sample t-test We use the energy data to illustrate the use of t.test for testing equality of population means based on two independent samples. Here, we wish to compare mean energy expenditure between lean and obese women. data ( energy ) ? energy attach ( energy ) energy What are the assumptions you need to check for carring out a test ? Answer # assumption 1: data in each group are normally distributed. ind.obese <- which ( energy $ stature == \"obese\" ) ind.lean <- which ( energy $ stature == \"lean\" ) shapiro_test ( energy $ expend [ ind.obese ]) shapiro_test ( energy $ expend [ ind.lean ]) # assumption 2: the variances for the two independent groups are equal. levene_test ( energy , expend ~ stature ) # if the command above does not work, use the levene test from the car package car :: leveneTest ( expend ~ stature , energy , center = \"median\" ) The variable stature gives the grouping. The test can be carried out as follows: t.test ( expend ~ stature ) Check that you understand the output. For an alpha level of 0.01, do you reject the null hypothesis? Exercise 3: Paired t-test Paired tests are used when there are two measurements (a \u2018pair\u2019) on the same individual. A paired test is essentially a one-sample test of the differences between the measurements. Any assumptions to be tested ? Answer # assumption 1: Each of the paired measurements must be obtained from the same subject # check your sampling design ! # assumption 2: The measured differences are normally distributed. intake.diff <- intake $ post - intake $ pre intake.diff.df <- as.data.frame ( intake.diff ) shapiro_test ( intake.diff ) We can carry out a paired t-test on the differences between pre and post from the intake data as follows: t.test ( pre , post , paired = TRUE ) Again, make sure that you know how to interpret the output. Assuming an alpha level of 0.01, what do you conclude? It was important here to tell t.test that this was a paired test. What happens if you leave out paired=TRUE from the t.test command? Are the assumptions for a two-sample test satisfied in this situation? Exercise 4: Simulation of mice weight data and p values We are going to simulate in a very simple way weight data for WT and KO mice. We will use a two-sample t-test for testing the difference in mean weight between the 2 groups of mice. KO <- runif ( 10 , min = 27 , max = 34 ) WT <- runif ( 10 , min = 27 , max = 34 ) KO <- as.data.frame ( KO ) names ( KO )[ 1 ] <- \"weight\" KO $ genotype <- \"KO\" WT <- as.data.frame ( WT ) names ( WT )[ 1 ] <- \"weight\" WT $ genotype <- \"WT\" KO_WT <- rbind ( KO , WT ) boxplot ( KO_WT $ weight ~ KO_WT $ genotype , main = \"Mice weight at 18 weeks\" , xlab = \"\" , ylab = \"\" ) res.welch.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype ) res.t.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype , var.equal = T ) Display or summarise the resulting p-values. We will now simulate weight data for WT and KO mice 1000 times and look at the distribution of p-values: sim.p.welch.test <- NULL sim.p.t.test <- NULL for ( i in 1 : 1000 ) { KO <- runif ( 10 , min = 27 , max = 34 ) WT <- runif ( 10 , min = 27 , max = 34 ) KO <- as.data.frame ( KO ) names ( KO )[ 1 ] <- \"weight\" KO $ genotype <- \"KO\" WT <- as.data.frame ( WT ) names ( WT )[ 1 ] <- \"weight\" WT $ genotype <- \"WT\" KO_WT <- rbind ( KO , WT ) res.welch.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype ) res.t.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype , var.equal = T ) sim.p.welch.test <- c ( sim.p.welch.test , res.welch.test $ p.value ) sim.p.t.test <- c ( sim.p.t.test , res.t.test $ p.value ) } sum ( sim.p.welch.test < 0.05 ) sum ( sim.p.t.test < 0.05 ) How many tests are significant ? What if you apply a Bonferroni correction ? What if you apply a FDR correction ? (use the p.adjust function if needed). Change the parameters of the simulations and see what is the effect on the p-values. Answer adj.bonf <- p.adjust ( sim.p.welch.test , method = \"bonf\" ) sum ( adj.bonf < 0.05 ) adj.BH <- p.adjust ( sim.p.welch.test , method = \"BH\" ) sum ( adj.BH < 0.05 ) Exercise 5: ANOVA Install the library faraway library ( faraway ) data ( coagulation ) The dataset comes from Faraway (2002) and comprises a set of 24 blood coagulation times. 24 animals were randomly assigned to four different diets and the samples were taken in a random order. Load the data and explore the dataset Answer data ( coagulation ) # check the data summary ( coagulation ) coagulation %>% group_by ( diet ) %>% get_summary_stats ( coag , type = \"mean_sd\" ) boxplot ( coagulation $ coag ~ coagulation $ diet ) ggboxplot ( coagulation , x = \"diet\" , y = \"coag\" ) Fit an ANOVA model, this also means checking assumptions! Answer # check normality ? ggqqplot ggqqplot ( coagulation [ coagulation $ diet == \"A\" ,], \"coag\" ) ggqqplot ( coagulation [ coagulation $ diet == \"B\" ,], \"coag\" ) ggqqplot ( coagulation [ coagulation $ diet == \"C\" ,], \"coag\" ) ggqqplot ( coagulation [ coagulation $ diet == \"D\" ,], \"coag\" ) shapiro.test ( coagulation [ coagulation $ diet == \"A\" , \"coag\" ]) shapiro.test ( coagulation [ coagulation $ diet == \"B\" , \"coag\" ]) shapiro.test ( coagulation [ coagulation $ diet == \"C\" , \"coag\" ]) shapiro.test ( coagulation [ coagulation $ diet == \"D\" , \"coag\" ]) # check variance equality levene_test ( coagulation , coag ~ diet ) # if the command above does not work, use the levene test from the car package car :: leveneTest ( coag ~ diet , coagulation , center = \"median\" ) # do anova anova_diet <- aov ( coagulation $ coag ~ coagulation $ diet ) summary ( anova_diet ) Is there some differences between the groups? If yes, which group(s) is different ? Answer # check pairwise TukeyHSD ( anova_diet ) pairwise.t.test ( coagulation $ coag , coagulation $ diet , p.adj = \"bonf\" ) pairwise.t.test ( coagulation $ coag , coagulation $ diet , p.adj = \"holm\" )","title":"Day 2"},{"location":"day2/#hypothesis-testing","text":"In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. Slides of lectures: Download slides Morning Download slides Afternoon The purpose of this exercise is to help you to better interpret a p-value by using R for introducing you to some simple hypothesis testing functions. As usual, be sure to read the help documentation for any new functions.","title":"Hypothesis Testing"},{"location":"day2/#exercise-1-one-sample-t-test","text":"We use the intake data, available in the ISwR package. We will use the variable pre. library ( ISwR ) library ( ggplot2 ) #we will show you how to do some standard ggplots data ( intake ) ? intake attach ( intake ) intake Start off by looking at some simple summary statistics: mean, sd, quantile (hardly necessary for such a small data set, but good practice). Answer summary ( intake ) mean ( intake $ pre ) sd ( intake $ pre ) Might these data be approximately normally distributed? Answer # Assumption: no significant outliers in the data ggboxplot ( intake $ pre , width = 0.5 , add = c ( \"mean\" , \"jitter\" ), ylab = \"premenstrual intake\" , xlab = F ) identify_outliers ( as.data.frame ( intake $ pre )) # Assumption: normality ggqqplot ( intake , \"pre\" ) shapiro_test ( intake $ pre ) Suppose you wish to test whether there is a systematic deviation between the women\u2019s (pre) energy intake and a recommended value of 7725 kJ. Assuming that the data are from a normal distribution, we are interested in testing whether the (population) mean is 7725. We can do a t-test in R as follows: t.test ( pre , mu = 7725 ) Any idea what the argument alternative is doing ? t.test ( pre , mu = 7725 , alternative = \"less\" ) There are several components to the output, Take some time to make sure you can understand what it all means. For an alpha level of 0.05, do you reject the null hypothesis? What about for an alpha level of 0.01? The default assumes that you want a 2-sided test. Use help to find out how you could get a 1-sided test that would be meaningful for the dataset, and carry this out. For an alpha level of 0.01, do you reject the null hypothesis?","title":"Exercise 1: One-sample t-test"},{"location":"day2/#exercise-2-two-sample-t-test","text":"We use the energy data to illustrate the use of t.test for testing equality of population means based on two independent samples. Here, we wish to compare mean energy expenditure between lean and obese women. data ( energy ) ? energy attach ( energy ) energy What are the assumptions you need to check for carring out a test ? Answer # assumption 1: data in each group are normally distributed. ind.obese <- which ( energy $ stature == \"obese\" ) ind.lean <- which ( energy $ stature == \"lean\" ) shapiro_test ( energy $ expend [ ind.obese ]) shapiro_test ( energy $ expend [ ind.lean ]) # assumption 2: the variances for the two independent groups are equal. levene_test ( energy , expend ~ stature ) # if the command above does not work, use the levene test from the car package car :: leveneTest ( expend ~ stature , energy , center = \"median\" ) The variable stature gives the grouping. The test can be carried out as follows: t.test ( expend ~ stature ) Check that you understand the output. For an alpha level of 0.01, do you reject the null hypothesis?","title":"Exercise 2: Two-sample t-test"},{"location":"day2/#exercise-3-paired-t-test","text":"Paired tests are used when there are two measurements (a \u2018pair\u2019) on the same individual. A paired test is essentially a one-sample test of the differences between the measurements. Any assumptions to be tested ? Answer # assumption 1: Each of the paired measurements must be obtained from the same subject # check your sampling design ! # assumption 2: The measured differences are normally distributed. intake.diff <- intake $ post - intake $ pre intake.diff.df <- as.data.frame ( intake.diff ) shapiro_test ( intake.diff ) We can carry out a paired t-test on the differences between pre and post from the intake data as follows: t.test ( pre , post , paired = TRUE ) Again, make sure that you know how to interpret the output. Assuming an alpha level of 0.01, what do you conclude? It was important here to tell t.test that this was a paired test. What happens if you leave out paired=TRUE from the t.test command? Are the assumptions for a two-sample test satisfied in this situation?","title":"Exercise 3: Paired t-test"},{"location":"day2/#exercise-4-simulation-of-mice-weight-data-and-p-values","text":"We are going to simulate in a very simple way weight data for WT and KO mice. We will use a two-sample t-test for testing the difference in mean weight between the 2 groups of mice. KO <- runif ( 10 , min = 27 , max = 34 ) WT <- runif ( 10 , min = 27 , max = 34 ) KO <- as.data.frame ( KO ) names ( KO )[ 1 ] <- \"weight\" KO $ genotype <- \"KO\" WT <- as.data.frame ( WT ) names ( WT )[ 1 ] <- \"weight\" WT $ genotype <- \"WT\" KO_WT <- rbind ( KO , WT ) boxplot ( KO_WT $ weight ~ KO_WT $ genotype , main = \"Mice weight at 18 weeks\" , xlab = \"\" , ylab = \"\" ) res.welch.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype ) res.t.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype , var.equal = T ) Display or summarise the resulting p-values. We will now simulate weight data for WT and KO mice 1000 times and look at the distribution of p-values: sim.p.welch.test <- NULL sim.p.t.test <- NULL for ( i in 1 : 1000 ) { KO <- runif ( 10 , min = 27 , max = 34 ) WT <- runif ( 10 , min = 27 , max = 34 ) KO <- as.data.frame ( KO ) names ( KO )[ 1 ] <- \"weight\" KO $ genotype <- \"KO\" WT <- as.data.frame ( WT ) names ( WT )[ 1 ] <- \"weight\" WT $ genotype <- \"WT\" KO_WT <- rbind ( KO , WT ) res.welch.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype ) res.t.test <- t.test ( KO_WT $ weight ~ KO_WT $ genotype , var.equal = T ) sim.p.welch.test <- c ( sim.p.welch.test , res.welch.test $ p.value ) sim.p.t.test <- c ( sim.p.t.test , res.t.test $ p.value ) } sum ( sim.p.welch.test < 0.05 ) sum ( sim.p.t.test < 0.05 ) How many tests are significant ? What if you apply a Bonferroni correction ? What if you apply a FDR correction ? (use the p.adjust function if needed). Change the parameters of the simulations and see what is the effect on the p-values. Answer adj.bonf <- p.adjust ( sim.p.welch.test , method = \"bonf\" ) sum ( adj.bonf < 0.05 ) adj.BH <- p.adjust ( sim.p.welch.test , method = \"BH\" ) sum ( adj.BH < 0.05 )","title":"Exercise 4: Simulation of mice weight data and p values"},{"location":"day2/#exercise-5-anova","text":"Install the library faraway library ( faraway ) data ( coagulation ) The dataset comes from Faraway (2002) and comprises a set of 24 blood coagulation times. 24 animals were randomly assigned to four different diets and the samples were taken in a random order. Load the data and explore the dataset Answer data ( coagulation ) # check the data summary ( coagulation ) coagulation %>% group_by ( diet ) %>% get_summary_stats ( coag , type = \"mean_sd\" ) boxplot ( coagulation $ coag ~ coagulation $ diet ) ggboxplot ( coagulation , x = \"diet\" , y = \"coag\" ) Fit an ANOVA model, this also means checking assumptions! Answer # check normality ? ggqqplot ggqqplot ( coagulation [ coagulation $ diet == \"A\" ,], \"coag\" ) ggqqplot ( coagulation [ coagulation $ diet == \"B\" ,], \"coag\" ) ggqqplot ( coagulation [ coagulation $ diet == \"C\" ,], \"coag\" ) ggqqplot ( coagulation [ coagulation $ diet == \"D\" ,], \"coag\" ) shapiro.test ( coagulation [ coagulation $ diet == \"A\" , \"coag\" ]) shapiro.test ( coagulation [ coagulation $ diet == \"B\" , \"coag\" ]) shapiro.test ( coagulation [ coagulation $ diet == \"C\" , \"coag\" ]) shapiro.test ( coagulation [ coagulation $ diet == \"D\" , \"coag\" ]) # check variance equality levene_test ( coagulation , coag ~ diet ) # if the command above does not work, use the levene test from the car package car :: leveneTest ( coag ~ diet , coagulation , center = \"median\" ) # do anova anova_diet <- aov ( coagulation $ coag ~ coagulation $ diet ) summary ( anova_diet ) Is there some differences between the groups? If yes, which group(s) is different ? Answer # check pairwise TukeyHSD ( anova_diet ) pairwise.t.test ( coagulation $ coag , coagulation $ diet , p.adj = \"bonf\" ) pairwise.t.test ( coagulation $ coag , coagulation $ diet , p.adj = \"holm\" )","title":"Exercise 5: ANOVA"},{"location":"day3/","text":"Linear Models In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. The purpose of these exercises is to introduce you to using R for regression modeling. Applications are the estimation of parameter values, the determination of variables that are associated with each other, for example the identification of important biological factors that affect a given \u201coutcome\u201d. Slides of lectures: Download slides Exercise class Go to the right directory and load the file class.csv class <- read.csv ( \"class.csv\" ) inspect the data dim ( class ) head ( class ) summary ( class [, -1 ]) Problem the data set does not have column of Gender as a factor class <- as.data.frame ( class ) class $ Gender <- as.factor ( class $ Gender ) inspect the pairs of variables on 2by2plots pairs ( class [, - c ( 1 )]) Investigating Height ~ Age without the attach function Height2 <- class $ Height Age2 <- class $ Age lm ( Height2 ~ Age2 ) or using the data slot lm ( Height ~ Age , data = class ) model <- lm ( Height ~ Age , data = class ) model plot the age vs Height and add the fitted line with abline plot ( class $ Age , class $ Height ) abline ( model , col = \"red\" , lwd = 2 ) change the range of xaxis and yaxis in order to visually see the intersept plot ( class $ Age , class $ Height , xlim = range ( 0 , class $ Age ), ylim = range ( coef ( model )[ 1 ], class $ Height )) abline ( model , col = \"red\" , lwd = 2 ) Do a summary of the linear model to see the residuals the degrees of freedom and the p-values estimated summary ( lm ( Height ~ Age , data = class ) ) Anscombe data We will start by exploring the classic Anscombe example data set which comes with R. You will soon see why it is \u2018instructive\u2019. It has been created for the didactic purpose to make people analysing data aware of the fact that while correlation is very convenient and useful, one should know certain limitations. This analysis will demonstrate the importance of examining scatterplots as part of data analysis. First we load the data. data ( anscombe ) This is a small dataset, so we can look at the whole data. For large datasets we would just look at the dimension (dim) of the data frame and at a few lines, here [1:3, ] selects the first three rows (and all their columns). Then we get some summary information for the 8 columns (four X,Y pairs), the summary function automatically executes on each column and returns the mean and the \u201cfive numbers summary\u201d (minimum, maximum, 1st 2nd and 3rd quartile, the 2nd is the median) Similarly, we make the boxplots, for a graphical visualization. anscombe dim ( anscombe ) anscombe [ 1 : 3 , ] summary ( anscombe ) boxplot ( anscombe ) Question Q1: What do you notice about the summary statistics? (Example answer given later) Now, we \u201cattach\u201d the data, we can then use the variable names used in the data (the names of the columns) directly in the R console window. attach ( anscombe ) Computing correlations; Correlations and scatterplots This data set consists of four pairs of X,Y data: x1 and y1 go together, etc. Find the correlation coefficient for each of the four sets. You can either do this for each pair separately by hand, e.g. cor ( x1 , y1 ) cor ( x2 , y2 ) cor ( x3 , y3 ) cor ( x4 , y4 ) etc., or all at once (cor applies to all pairwise combinations of columns), and the result of cor is given to the function round specifying that we want rounding to 3 decimal places. round ( cor ( anscombe ) , digits = 3 ) Anything interesting here? Guess what the four scatterplots will look like.... Done guessing? :) OK, now for the test - go ahead and make the four scatterplots. The first command subdivides the graphical window into 2x2 panels, each with a square plotting region. par ( mfrow = c ( 2 , 2 ), pty = \"s\" ) plot ( x1 , y1 ) plot ( x2 , y2 ) plot ( x3 , y3 ) plot ( x4 , y4 ) Were you right? Are you surprised?? Linear regressions We can add regression lines (abline plots straight lines) and a title to the plots as follows: plot ( x1 , y1 ) abline ( lm ( y1 ~ x1 )) title ( \"Plot 1\" ) plot ( x2 , y2 ) abline ( lm ( y2 ~ x2 )) title ( \"Plot 2\" ) plot ( x3 , y3 ) abline ( lm ( y3 ~ x3 )) title ( \"Plot 3\" ) plot ( x4 , y4 ) abline ( lm ( y4 ~ x4 )) title ( \"Plot 4\" ) Moral of the story: there is only one way to tell what the scatterplot will look like: you have to look at it! Even the results of the statistical estimation of intercept and slope, based on assumed normal distribution of the residuals, can be misleading. Question Q2: Which of the data correspond to which of the four comments?: A) One single point drives the correlation to a higher value B) One single point drives the correlation to a lower value C) A case where the straight line seems the appropriate model D) The graph departs from a straight line, shows curvature, the straight line seems unsuitable in the sense that a better model exists. What do you expect for the estimation of the slopes? Will the P values and the confidence intervals be comparable? The summary function applied to the object returnd by lm summary ( lm ( y1 ~ x1 )) summary ( lm ( y2 ~ x2 )) summary ( lm ( y3 ~ x3 )) summary ( lm ( y4 ~ x4 )) Here one could compute hat values to see if they can identify points with a high influence on the estimates. But this is planned already for another example. Question Q3: Can you find a more appropriate model for (x2,y2) ? Don\u2019t open this before having tried to answer Solution to Questions: Q1: With the exception of x4, the x columns have almost identical numerical summaries, and so have the y columns. Q2: Pairs Description-Plot: A-4, B-3, C-1, D-2 The first plot seems to be distributed normally, and corresponds to what one would expect when considering two variables correlated and following the assumption of normality. The second one is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear, and the Pearson correlation coefficient is not relevant. In the third case, the linear relationship is perfect, except for one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.81. Finally, the fourth example shows another example when one outlier is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear. Q3: The graph resembles that of a quadratic function. We can use a polynomial including both x and x^2: z2 <- x2 ^ 2 summary ( lm ( y2 ~ x2 + z2 )) The summary shows that the results of these models fit better to the given points. You cannot use the abline function to plot the resulting curve on the plot, as it is not a straight line anymore. However, you can use the curve function, using the three coefficients returned by the summary: plot ( x2 , y2 ) curve ( -5.99+2.78 * x -0.127 * x ^ 2 , add = TRUE ) Alternatively, you can extract the coefficients directly: coefs <- coef ( lm ( y2 ~ x2 + z2 ) ) curve ( coefs [ 1 ] + coefs [ 2 ] * x + coefs [ 3 ] * x ^ 2 , add = TRUE ) Thuesen data Exploring bivariate data We will use the \u2018Thuesen\u2019 data (L. Thuesen, Diabetologica 1985) contained in the ISwR package. (book \u201cIntroductory Statistics with R\u201d by P. Dalgaard). This data set contains fasting blood glucose concentration (mmol/l, normal range 4-6 mM, except shortly after eating) and mean circumferential shortening velocity of the left ventriculum (percentages of circumference / s, average in the healthy control group was 0.85) measured for 24 (type 1-) diabetic patients. Both variables are numeric, continuous. Diabetes is considered a proven a risk factor for heart disease. The glucose concentration is increased and the degree of increase is an indicator of how well the person is managing to control it (\u2018metabolic control\u2019), for example by controlling sugar uptake. The shortening velocity is an indicator of cardiac function. It is increased in diabetic persons and believed to be a risk factor for heart valve function impairment. A question of interest here is if better metabolic control - as shown by lower (more normal) blood glucose level - is associated with lower (more normal) shortening velocity. The interest was in testing, if there is a relation between these two physiologial quantities. If so, it would suggest that a diabetic person that can keep its glucose low might have less risk of suffering later from heart diseases. library ( ISwR ) data ( thuesen ) We can start by looking at the data: (You would not want to do this for very large data sets!) thuesen There are several ways to access the individual variables in thuesen: We could use the $ operator: \u2018name of the data - $ - name of the variable\u2019: thuesen $ short.velocity thuesen $ blood.glucose Or we could use the subset operator [ to select individual columns by name or by position: thuesen [, \"blood.glucose\" ] thuesen [, 2 ] Or we could attach the data and just refer to the variables by their names: attach ( thuesen ) short.velocity blood.glucose This is still a lot of typing! We could assign the variables to new ones with shorter names: sv <- thuesen $ short.velocity bg <- thuesen $ blood.glucose Get some univariate summaries summary ( thuesen ) If you are interested in knowing just the means, you can use the function colMeans(thuesen). You might have noticed that one of the observations was \u2018NA\u2019 (i.e. Not Available). In case you did not notice this, it will be signaled to you if you use certain functions on the data (e.g. mean, sd). You must specify that you want the observations with NA values removed: colMeans ( thuesen , na.rm = TRUE ) Look at a bivariate summary If you type: cor ( thuesen ) you will get an error: Error in cor(thuesen) : missing observations in cov/cor. [You will have seen a similar message above when calculating the means.] This is because of the missing value. We can learn which argument to set in cor to decide how to handle the missing values, by using the help facility \u2018?\u2019 to ask for the documentation for this function. Here we use the \u2018use\u2019 argument to exclude those observations that have one or more missing values from the calculation of the correlation. ? cor round ( cor ( thuesen , use = \"complete.obs\" ), 4 ) Q1) What is the correlation between blood glucose and shortening velocity? What do you think the scatter plot will look like? Well, we just learned that the only way to know is to look! Look at the scatter plot We will be interested in predicting shortening velocity from blood glucose, so our Y variable is sv and X variable is bg: plot ( bg , sv ) We can enhance the plot in a number of ways, for example changing the plotting character (pch), adding different axis labels (xlab, ylab): plot ( bg , sv , pch = 16 , xlab = \"Blood Glucose\" , ylab = \"Shortening Velocity\" ) Finding the Regression Line Use lm to estimate the regression line from the data: th.lm <- lm ( sv ~ bg ) We can get enough information to write out the regression line from: th.lm However, we will see a more comprehensive summary using \u2018summary\u2019 : summary ( th.lm ) Q2) Using this information, write out a formula for the regression line. Are any coefficients significant at the 5% level? (could you estimate a 95% confidence interval?) Is sv predicted to be lower when bg is lower, or not? We can add the regression line to the scatter plot: abline ( lm ( sv ~ bg )) Exploring Model Fit To explore the fit of the model, we should examine the residuals more carefully. First, extract the residuals from th.lm using the \u2018extractor\u2019 resid function: th.resid <- resid ( th.lm ) Normality First, extract the residuals from th.lm. We eliminate the observations with missing values for sv (using is.na(sv)) and the \u2018not\u2019 operator \u2018!\u2019. Then we produce QQ plots for the residuals. bg.1 <- bg [ ! is.na ( sv )] plot ( bg.1 , th.resid ) abline ( h = 0 ) qqnorm ( th.resid ) qqline ( th.resid ) Q3) Are any points off the line to indicate departure from a normal distribution? Constant variance We check this assumption by plotting the residuals against the fitted values that we obtain by applying the function of this name to the lm object. th.fv <- fitted.values ( th.lm ) plot ( th.fv , th.resid ) abline ( h = 0 ) Q4) Does the variance of the residuals seem roughly constant across the range of x? If not, what pattern do you see? Influential points Here we compute the \u2018hat\u2019 values (function lm.influence, variable hat in the object returned) We want to compare the hat values to 2p/n or 3p/n, where p is the dimension of the model space (2 here). Values bigger than this may be considered \u2018influential\u2019. Note the use of \u2018#\u2019, this symbol and the rest of the line are ignored by R. It is useful to add small comments to our R code (and to keep this together in a protocol file). th.hat <- lm.influence ( th.lm ) sort ( th.hat $ hat ) # look at the sorted values index <- seq ( 1 : length ( th.hat $ hat )) # integers 1 .. number of points plot ( index , th.hat $ hat , xlab = \"Index\" , ylab = \"Hat value\" , ylim = c ( 0 , 0.3 )) # ylim sets the range of the y-xis abline ( h = c ( 2 * 2 / 23 , 3 * 2 / 23 ), lty = c ( 2 , 3 ), col = c ( \"blue\" , \"red\" ) ) # h for horizontal lines, here two specified together Q5) Do there appear to be any influential points beyond the two horizontal lines? We can find measurements corresponding to the highest leverage points as follows, using R \u2018interactive\u2019 capabilities, write: th.highlev <- identify ( index , th.hat $ hat ) and then click on the two highest points in the graphical window (they should be at index 4 and 13). When you are finished, return t the R console window and see which points they are: th.highlev # should be 4 , 13 We can get the glucose and velocity measurements for these points by typing thuesen [ th.highlev ,] Let\u2019s see where these points are on the original scatter plot. We\u2019ll plot these points as large (scale \u2018cex=2\u2019) blue dots: plot ( bg , sv , pch = 16 , xlab = \"Blood Glucose\" , ylab = \"Shortening Velocity\" ) abline ( lm ( sv ~ bg )) points ( bg [ th.highlev ], sv [ th.highlev ], pch = 16 , col = \"blue\" , cex = 2 ) Why do you think these points are the most influential? Don\u2019t open before having tried to answer Q1) cor r = 0.4168 Q2) The results indicate high significance for a test of the hypothesis \u2018intercept = 0\u2019 and are just barely significant for a test on the \u2018coefficient = 0\u2019 for bg. An association between bg and sv can be assumed given these data, but the evidence is not overwhelming strong and caution in interpretation is required, especially if there might be doubts as to the correctness of the model assumption being made. regression line: sv = 1.09781 + 0.02196 * bg predicts on average an increase of sv with bg Estimated 95% CI for the coefficients of bg: Half-width: 1.96 * 0.01045 = 0.0205 (using the 1.96 from the normal distribution) CI: 0.0222 +/- 0.0205 = 0.0017, 0.0427 Half-width: 2.08 * 0.01045 = 0.0217 (using the t distribution with 21 degrees of freedom) CI: 0.0222 +/- 0.0217 = 0.0005, 0.0439 The slope coefficient is significant in the hypothesis test agains slope=0, but the confidence intervals include values very close to 0. While it is predicted that lower bg is associated with lover sv, how strong this effect is remains relatively unclear. Q3) The five points on the right hand side are fairly far from the straight line and suggest a departure from normal distribution that is important enough to be cautious about drawing definitive conclusions. Q4) The variance seems roughly constant, but a bit higher on the right hand side for the larger fitted values. There can be a bit of concern about model validity. Q5) There are two points with a high influence (leverage) They are obove the 2p/n but below the 3p/n line. Again, this is a reason of concern. In summary, the data present evidence for a linear association of higher ventricular contraction velocity when blood glucose is higher, but with substantial unexplained variation in the model (explained variance by R squared below 0.2, test on coefficient for bg borderline to significance). Because the data suggest a relation, but no final conclusion can be strongly supported, the recommendation is to sample more observations. In particular the relation is not well estimated at higher values of bg, and more observations with bg > 13 would be useful. Expression data We are going to use data from a breast cancer study, described in the paper by C. Sotiriou et al, \u201cGene expression profiling in breast cancer: understanding the molecular basis of histological grade to improve prognosis\u201d, J Natl Cancer Inst (2006 Feb 15;98(4):262-72). File expression-esr1.csv contains microarray data for 20 different patients and 9 different probes. The 9 probes interrogate the same gene, ESR1 (Estrogen Receptor). The expression data is log transformed. File clindata.csv contains clinical data about these 20 patients. In particular, the \u201cer\u201d column indicates whether the tumour was identified (using immunohistochemistry) as expression an estrogen receptor. In theory, this information at the protein level should match the information obtained from RNA: an er value of 1 should correspond to a high expression value for gene ESR1, while a value of 0 should correspond to a low value. Answer the following questions (using statistical and graphical arguments). Do the 9 probes give similar results, or can you identify one or several clusters of probes that give similar results ? Do the probes give similar results than those obtained by immunohistochemistry ? What is the probe that is most closely associated with the immunihistochemistry data ? You can use the cor() command on a table of variables and which will calculate all pairwise correlations; the pairs() command will create the related graphics. Answer ##open it and look at it clin <- read.csv ( \"clindata.csv\" ) expr <- read.csv ( \"expression-esr1.csv\" ) head ( clin ) summary ( clin ) clin $ er <- as.factor ( clin $ er ) clin $ treatment <- as.factor ( clin $ treatment ) rownames ( clin ) <- clin [, 1 ] summary ( clin ) head ( expr ) dim ( expr ) rownames ( expr ) <- expr [, 1 ] ##check all the correlation (Pearson) and see which one correspond cor ( expr ) ##for loop to see all the correlations with ER status for ( i in 1 : 9 ){ print ( cor ( expr [, i ], as.numeric ( clin [ rownames ( expr ), \"er\" ]), method = \"spearman\" )) } ### put them in a variable s <- c () for ( i in 1 : 9 ){ s [ i ] <- cor ( expr [, i ], as.numeric ( clin [ rownames ( expr ), \"er\" ]), method = \"spearman\" ) } head ( clin ) ### do some boxplots plot ( clin $ er , expr [, 1 ])","title":"Day 3"},{"location":"day3/#linear-models","text":"In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. The purpose of these exercises is to introduce you to using R for regression modeling. Applications are the estimation of parameter values, the determination of variables that are associated with each other, for example the identification of important biological factors that affect a given \u201coutcome\u201d. Slides of lectures: Download slides","title":"Linear Models"},{"location":"day3/#exercise-class","text":"Go to the right directory and load the file class.csv class <- read.csv ( \"class.csv\" ) inspect the data dim ( class ) head ( class ) summary ( class [, -1 ]) Problem the data set does not have column of Gender as a factor class <- as.data.frame ( class ) class $ Gender <- as.factor ( class $ Gender ) inspect the pairs of variables on 2by2plots pairs ( class [, - c ( 1 )]) Investigating Height ~ Age without the attach function Height2 <- class $ Height Age2 <- class $ Age lm ( Height2 ~ Age2 ) or using the data slot lm ( Height ~ Age , data = class ) model <- lm ( Height ~ Age , data = class ) model plot the age vs Height and add the fitted line with abline plot ( class $ Age , class $ Height ) abline ( model , col = \"red\" , lwd = 2 ) change the range of xaxis and yaxis in order to visually see the intersept plot ( class $ Age , class $ Height , xlim = range ( 0 , class $ Age ), ylim = range ( coef ( model )[ 1 ], class $ Height )) abline ( model , col = \"red\" , lwd = 2 ) Do a summary of the linear model to see the residuals the degrees of freedom and the p-values estimated summary ( lm ( Height ~ Age , data = class ) )","title":"Exercise class"},{"location":"day3/#anscombe-data","text":"We will start by exploring the classic Anscombe example data set which comes with R. You will soon see why it is \u2018instructive\u2019. It has been created for the didactic purpose to make people analysing data aware of the fact that while correlation is very convenient and useful, one should know certain limitations. This analysis will demonstrate the importance of examining scatterplots as part of data analysis. First we load the data. data ( anscombe ) This is a small dataset, so we can look at the whole data. For large datasets we would just look at the dimension (dim) of the data frame and at a few lines, here [1:3, ] selects the first three rows (and all their columns). Then we get some summary information for the 8 columns (four X,Y pairs), the summary function automatically executes on each column and returns the mean and the \u201cfive numbers summary\u201d (minimum, maximum, 1st 2nd and 3rd quartile, the 2nd is the median) Similarly, we make the boxplots, for a graphical visualization. anscombe dim ( anscombe ) anscombe [ 1 : 3 , ] summary ( anscombe ) boxplot ( anscombe ) Question Q1: What do you notice about the summary statistics? (Example answer given later) Now, we \u201cattach\u201d the data, we can then use the variable names used in the data (the names of the columns) directly in the R console window. attach ( anscombe ) Computing correlations; Correlations and scatterplots This data set consists of four pairs of X,Y data: x1 and y1 go together, etc. Find the correlation coefficient for each of the four sets. You can either do this for each pair separately by hand, e.g. cor ( x1 , y1 ) cor ( x2 , y2 ) cor ( x3 , y3 ) cor ( x4 , y4 ) etc., or all at once (cor applies to all pairwise combinations of columns), and the result of cor is given to the function round specifying that we want rounding to 3 decimal places. round ( cor ( anscombe ) , digits = 3 ) Anything interesting here? Guess what the four scatterplots will look like.... Done guessing? :) OK, now for the test - go ahead and make the four scatterplots. The first command subdivides the graphical window into 2x2 panels, each with a square plotting region. par ( mfrow = c ( 2 , 2 ), pty = \"s\" ) plot ( x1 , y1 ) plot ( x2 , y2 ) plot ( x3 , y3 ) plot ( x4 , y4 ) Were you right? Are you surprised??","title":"Anscombe data"},{"location":"day3/#linear-regressions","text":"We can add regression lines (abline plots straight lines) and a title to the plots as follows: plot ( x1 , y1 ) abline ( lm ( y1 ~ x1 )) title ( \"Plot 1\" ) plot ( x2 , y2 ) abline ( lm ( y2 ~ x2 )) title ( \"Plot 2\" ) plot ( x3 , y3 ) abline ( lm ( y3 ~ x3 )) title ( \"Plot 3\" ) plot ( x4 , y4 ) abline ( lm ( y4 ~ x4 )) title ( \"Plot 4\" ) Moral of the story: there is only one way to tell what the scatterplot will look like: you have to look at it! Even the results of the statistical estimation of intercept and slope, based on assumed normal distribution of the residuals, can be misleading. Question Q2: Which of the data correspond to which of the four comments?: A) One single point drives the correlation to a higher value B) One single point drives the correlation to a lower value C) A case where the straight line seems the appropriate model D) The graph departs from a straight line, shows curvature, the straight line seems unsuitable in the sense that a better model exists. What do you expect for the estimation of the slopes? Will the P values and the confidence intervals be comparable? The summary function applied to the object returnd by lm summary ( lm ( y1 ~ x1 )) summary ( lm ( y2 ~ x2 )) summary ( lm ( y3 ~ x3 )) summary ( lm ( y4 ~ x4 )) Here one could compute hat values to see if they can identify points with a high influence on the estimates. But this is planned already for another example. Question Q3: Can you find a more appropriate model for (x2,y2) ? Don\u2019t open this before having tried to answer Solution to Questions: Q1: With the exception of x4, the x columns have almost identical numerical summaries, and so have the y columns. Q2: Pairs Description-Plot: A-4, B-3, C-1, D-2 The first plot seems to be distributed normally, and corresponds to what one would expect when considering two variables correlated and following the assumption of normality. The second one is not distributed normally; while an obvious relationship between the two variables can be observed, it is not linear, and the Pearson correlation coefficient is not relevant. In the third case, the linear relationship is perfect, except for one outlier which exerts enough influence to lower the correlation coefficient from 1 to 0.81. Finally, the fourth example shows another example when one outlier is enough to produce a high correlation coefficient, even though the relationship between the two variables is not linear. Q3: The graph resembles that of a quadratic function. We can use a polynomial including both x and x^2: z2 <- x2 ^ 2 summary ( lm ( y2 ~ x2 + z2 )) The summary shows that the results of these models fit better to the given points. You cannot use the abline function to plot the resulting curve on the plot, as it is not a straight line anymore. However, you can use the curve function, using the three coefficients returned by the summary: plot ( x2 , y2 ) curve ( -5.99+2.78 * x -0.127 * x ^ 2 , add = TRUE ) Alternatively, you can extract the coefficients directly: coefs <- coef ( lm ( y2 ~ x2 + z2 ) ) curve ( coefs [ 1 ] + coefs [ 2 ] * x + coefs [ 3 ] * x ^ 2 , add = TRUE )","title":"Linear regressions"},{"location":"day3/#thuesen-data","text":"Exploring bivariate data We will use the \u2018Thuesen\u2019 data (L. Thuesen, Diabetologica 1985) contained in the ISwR package. (book \u201cIntroductory Statistics with R\u201d by P. Dalgaard). This data set contains fasting blood glucose concentration (mmol/l, normal range 4-6 mM, except shortly after eating) and mean circumferential shortening velocity of the left ventriculum (percentages of circumference / s, average in the healthy control group was 0.85) measured for 24 (type 1-) diabetic patients. Both variables are numeric, continuous. Diabetes is considered a proven a risk factor for heart disease. The glucose concentration is increased and the degree of increase is an indicator of how well the person is managing to control it (\u2018metabolic control\u2019), for example by controlling sugar uptake. The shortening velocity is an indicator of cardiac function. It is increased in diabetic persons and believed to be a risk factor for heart valve function impairment. A question of interest here is if better metabolic control - as shown by lower (more normal) blood glucose level - is associated with lower (more normal) shortening velocity. The interest was in testing, if there is a relation between these two physiologial quantities. If so, it would suggest that a diabetic person that can keep its glucose low might have less risk of suffering later from heart diseases. library ( ISwR ) data ( thuesen ) We can start by looking at the data: (You would not want to do this for very large data sets!) thuesen There are several ways to access the individual variables in thuesen: We could use the $ operator: \u2018name of the data - $ - name of the variable\u2019: thuesen $ short.velocity thuesen $ blood.glucose Or we could use the subset operator [ to select individual columns by name or by position: thuesen [, \"blood.glucose\" ] thuesen [, 2 ] Or we could attach the data and just refer to the variables by their names: attach ( thuesen ) short.velocity blood.glucose This is still a lot of typing! We could assign the variables to new ones with shorter names: sv <- thuesen $ short.velocity bg <- thuesen $ blood.glucose Get some univariate summaries summary ( thuesen ) If you are interested in knowing just the means, you can use the function colMeans(thuesen). You might have noticed that one of the observations was \u2018NA\u2019 (i.e. Not Available). In case you did not notice this, it will be signaled to you if you use certain functions on the data (e.g. mean, sd). You must specify that you want the observations with NA values removed: colMeans ( thuesen , na.rm = TRUE ) Look at a bivariate summary If you type: cor ( thuesen ) you will get an error: Error in cor(thuesen) : missing observations in cov/cor. [You will have seen a similar message above when calculating the means.] This is because of the missing value. We can learn which argument to set in cor to decide how to handle the missing values, by using the help facility \u2018?\u2019 to ask for the documentation for this function. Here we use the \u2018use\u2019 argument to exclude those observations that have one or more missing values from the calculation of the correlation. ? cor round ( cor ( thuesen , use = \"complete.obs\" ), 4 ) Q1) What is the correlation between blood glucose and shortening velocity? What do you think the scatter plot will look like? Well, we just learned that the only way to know is to look! Look at the scatter plot We will be interested in predicting shortening velocity from blood glucose, so our Y variable is sv and X variable is bg: plot ( bg , sv ) We can enhance the plot in a number of ways, for example changing the plotting character (pch), adding different axis labels (xlab, ylab): plot ( bg , sv , pch = 16 , xlab = \"Blood Glucose\" , ylab = \"Shortening Velocity\" ) Finding the Regression Line Use lm to estimate the regression line from the data: th.lm <- lm ( sv ~ bg ) We can get enough information to write out the regression line from: th.lm However, we will see a more comprehensive summary using \u2018summary\u2019 : summary ( th.lm ) Q2) Using this information, write out a formula for the regression line. Are any coefficients significant at the 5% level? (could you estimate a 95% confidence interval?) Is sv predicted to be lower when bg is lower, or not? We can add the regression line to the scatter plot: abline ( lm ( sv ~ bg )) Exploring Model Fit To explore the fit of the model, we should examine the residuals more carefully. First, extract the residuals from th.lm using the \u2018extractor\u2019 resid function: th.resid <- resid ( th.lm ) Normality First, extract the residuals from th.lm. We eliminate the observations with missing values for sv (using is.na(sv)) and the \u2018not\u2019 operator \u2018!\u2019. Then we produce QQ plots for the residuals. bg.1 <- bg [ ! is.na ( sv )] plot ( bg.1 , th.resid ) abline ( h = 0 ) qqnorm ( th.resid ) qqline ( th.resid ) Q3) Are any points off the line to indicate departure from a normal distribution? Constant variance We check this assumption by plotting the residuals against the fitted values that we obtain by applying the function of this name to the lm object. th.fv <- fitted.values ( th.lm ) plot ( th.fv , th.resid ) abline ( h = 0 ) Q4) Does the variance of the residuals seem roughly constant across the range of x? If not, what pattern do you see? Influential points Here we compute the \u2018hat\u2019 values (function lm.influence, variable hat in the object returned) We want to compare the hat values to 2p/n or 3p/n, where p is the dimension of the model space (2 here). Values bigger than this may be considered \u2018influential\u2019. Note the use of \u2018#\u2019, this symbol and the rest of the line are ignored by R. It is useful to add small comments to our R code (and to keep this together in a protocol file). th.hat <- lm.influence ( th.lm ) sort ( th.hat $ hat ) # look at the sorted values index <- seq ( 1 : length ( th.hat $ hat )) # integers 1 .. number of points plot ( index , th.hat $ hat , xlab = \"Index\" , ylab = \"Hat value\" , ylim = c ( 0 , 0.3 )) # ylim sets the range of the y-xis abline ( h = c ( 2 * 2 / 23 , 3 * 2 / 23 ), lty = c ( 2 , 3 ), col = c ( \"blue\" , \"red\" ) ) # h for horizontal lines, here two specified together Q5) Do there appear to be any influential points beyond the two horizontal lines? We can find measurements corresponding to the highest leverage points as follows, using R \u2018interactive\u2019 capabilities, write: th.highlev <- identify ( index , th.hat $ hat ) and then click on the two highest points in the graphical window (they should be at index 4 and 13). When you are finished, return t the R console window and see which points they are: th.highlev # should be 4 , 13 We can get the glucose and velocity measurements for these points by typing thuesen [ th.highlev ,] Let\u2019s see where these points are on the original scatter plot. We\u2019ll plot these points as large (scale \u2018cex=2\u2019) blue dots: plot ( bg , sv , pch = 16 , xlab = \"Blood Glucose\" , ylab = \"Shortening Velocity\" ) abline ( lm ( sv ~ bg )) points ( bg [ th.highlev ], sv [ th.highlev ], pch = 16 , col = \"blue\" , cex = 2 ) Why do you think these points are the most influential? Don\u2019t open before having tried to answer Q1) cor r = 0.4168 Q2) The results indicate high significance for a test of the hypothesis \u2018intercept = 0\u2019 and are just barely significant for a test on the \u2018coefficient = 0\u2019 for bg. An association between bg and sv can be assumed given these data, but the evidence is not overwhelming strong and caution in interpretation is required, especially if there might be doubts as to the correctness of the model assumption being made. regression line: sv = 1.09781 + 0.02196 * bg predicts on average an increase of sv with bg Estimated 95% CI for the coefficients of bg: Half-width: 1.96 * 0.01045 = 0.0205 (using the 1.96 from the normal distribution) CI: 0.0222 +/- 0.0205 = 0.0017, 0.0427 Half-width: 2.08 * 0.01045 = 0.0217 (using the t distribution with 21 degrees of freedom) CI: 0.0222 +/- 0.0217 = 0.0005, 0.0439 The slope coefficient is significant in the hypothesis test agains slope=0, but the confidence intervals include values very close to 0. While it is predicted that lower bg is associated with lover sv, how strong this effect is remains relatively unclear. Q3) The five points on the right hand side are fairly far from the straight line and suggest a departure from normal distribution that is important enough to be cautious about drawing definitive conclusions. Q4) The variance seems roughly constant, but a bit higher on the right hand side for the larger fitted values. There can be a bit of concern about model validity. Q5) There are two points with a high influence (leverage) They are obove the 2p/n but below the 3p/n line. Again, this is a reason of concern. In summary, the data present evidence for a linear association of higher ventricular contraction velocity when blood glucose is higher, but with substantial unexplained variation in the model (explained variance by R squared below 0.2, test on coefficient for bg borderline to significance). Because the data suggest a relation, but no final conclusion can be strongly supported, the recommendation is to sample more observations. In particular the relation is not well estimated at higher values of bg, and more observations with bg > 13 would be useful.","title":"Thuesen data"},{"location":"day3/#expression-data","text":"We are going to use data from a breast cancer study, described in the paper by C. Sotiriou et al, \u201cGene expression profiling in breast cancer: understanding the molecular basis of histological grade to improve prognosis\u201d, J Natl Cancer Inst (2006 Feb 15;98(4):262-72). File expression-esr1.csv contains microarray data for 20 different patients and 9 different probes. The 9 probes interrogate the same gene, ESR1 (Estrogen Receptor). The expression data is log transformed. File clindata.csv contains clinical data about these 20 patients. In particular, the \u201cer\u201d column indicates whether the tumour was identified (using immunohistochemistry) as expression an estrogen receptor. In theory, this information at the protein level should match the information obtained from RNA: an er value of 1 should correspond to a high expression value for gene ESR1, while a value of 0 should correspond to a low value. Answer the following questions (using statistical and graphical arguments). Do the 9 probes give similar results, or can you identify one or several clusters of probes that give similar results ? Do the probes give similar results than those obtained by immunohistochemistry ? What is the probe that is most closely associated with the immunihistochemistry data ? You can use the cor() command on a table of variables and which will calculate all pairwise correlations; the pairs() command will create the related graphics. Answer ##open it and look at it clin <- read.csv ( \"clindata.csv\" ) expr <- read.csv ( \"expression-esr1.csv\" ) head ( clin ) summary ( clin ) clin $ er <- as.factor ( clin $ er ) clin $ treatment <- as.factor ( clin $ treatment ) rownames ( clin ) <- clin [, 1 ] summary ( clin ) head ( expr ) dim ( expr ) rownames ( expr ) <- expr [, 1 ] ##check all the correlation (Pearson) and see which one correspond cor ( expr ) ##for loop to see all the correlations with ER status for ( i in 1 : 9 ){ print ( cor ( expr [, i ], as.numeric ( clin [ rownames ( expr ), \"er\" ]), method = \"spearman\" )) } ### put them in a variable s <- c () for ( i in 1 : 9 ){ s [ i ] <- cor ( expr [, i ], as.numeric ( clin [ rownames ( expr ), \"er\" ]), method = \"spearman\" ) } head ( clin ) ### do some boxplots plot ( clin $ er , expr [, 1 ])","title":"Expression data"},{"location":"day4/","text":"Clustering and PCA In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. Slides of lectures: Download slides The Iris data set We will study the iris data set, containing measurements of the petal length and width and the sepal length and width for 150 iris flowers of three different types. The data set is available within R. First, we load the data, display its structure and summarize its content. data ( iris ) str ( iris ) summary ( iris ) head ( iris ) Are the range of values for the individual variables comparable? What is the dimensionality of the data (the number of coordinates used to represent the samples)? Next, we create univariate plots to visualize the variables individually or pairwise. boxplot ( iris [, 1 : 4 ]) pairs ( iris [, 1 : 4 ], col = iris $ Species , pch = 19 ) What do the colors represent? Are there any apparent associations between the variables? We can also create boxplots of the individual variables stratified by iris type. boxplot ( iris $ Petal.Width ~ iris $ Species ) Now, we apply PCA to the data, using the prcomp function. Make sure that you know what the arguments to the function stand for. pca.iris.cov = prcomp ( iris [, 1 : 4 ], center = TRUE , scale. = FALSE ) plot ( pca.iris.cov $ x , col = iris $ Species , pch = 19 ) What can you see in the sample plot? In addition to the sample configuration, we also look at the variable loadings, to see which variables contribute most to the principal components, and at the fraction of variance that is explained by each principal component. pca.iris.cov $ rotation summary ( pca.iris.cov ) Which variables have the strongest influence on each of the first two principal components? How can you use this information to interpret the sample PCA representation? We can now visualize the sample scores and the variable loadings together in a biplot. biplot ( pca.iris.cov , scale = 0 ) What do the different objects in the biplot represent? How are they connected to the output from prcomp? The sample scores from PCA are obtained as linear combinations of the four measured variables, with weights given by the variable loadings. Verify that this is the case e.g. for the scores for the first flower. iris.centered = scale ( iris [, 1 : 4 ], center = TRUE , scale = FALSE ) # compute linear combination scores.sample1 = iris.centered[1, ] %*% pca.iris.cov$rotation # compare to PCA scores pca.iris.cov $ x [ 1 , ] / scores.sample1 The PCA just applied was based on the non-standardized variables, that is, on the covariance matrix. Where was this specified? Can we gain some understanding of the results (the variable loadings) by studying the individual variable variances? var ( iris [, 1 : 4 ]) Next, we rerun the analysis with standardized variables. pca.iris.corr = prcomp ( iris [, 1 : 4 ], center = TRUE , scale. = TRUE ) plot ( pca.iris.corr $ x , col = iris $ Species , pch = 19 ) pca.iris.corr $ rotation summary ( pca.iris.corr ) biplot ( pca.iris.corr , scale = 0 ) How did the results change? Can we understand the change by using the information form the pairwise scatterplots and the individual variances? Which variables appear to be most important for separating the two groups of iris flowers seen in the PCA plot? To see the amount of variance captured by each of the principal components, construct scree plots for the two PCAs. par ( mfrow = c ( 1 , 2 )) screeplot ( pca.iris.cov , type = \"line\" ) screeplot ( pca.iris.corr , type = \"line\" ) How many principal components are needed to explain most of the variance in the data? Dengue fever example In this example, we will use a gene expression data set aimed at studying the differences between patients with dengue fever and healthy controls or convalescents. The data set corresponds to a publication by Kwissa et al (Cell Host & Microbe 16:115-127 (2014)), and the expression matrix has been deposited in Gene Expression Omnibus (GEO) with accession number GDS5093. Data retrieval We will use the GEOquery package to retrieve the data and platform annotation information. The GEOquery package is available from Bioconductor, and can be installed in the following way: source ( \"https://bioconductor.org/biocLite.R\" ) biocLite ( \"GEOquery\" ) And the data retrieved like this and then have a look at the downloaded data : library ( GEOquery ) ## Retrieve the data from GEO gds <- getGEO ( \"GDS5093\" ) ## If you have already downloaded the data, you can load the soft file directly ## gds <- getGEO(\"GDS5093.soft.gz\") ## Look at the elements of the downloaded data head(Columns(gds)) head ( Table ( gds )) head ( Meta ( gds )) ## Convert the gds object to an expression set eset <- GDS2eSet ( gds ) We have converted the downloaded object to a so called expression set. The expression set is a special Bioconductor data container that is commonly used to store all information associated with a microarray experiment. Using different accessor functions, we can extract the different types of information stored in the expression set (expression values, sample annotations, variable annotations) ## Sample information head ( pData ( eset )) ## Expression matrix head ( exprs ( eset )) ## Feature information head ( fData ( eset )) Principal component analysis First we perform a PCA using all the variables in the data set. We will perform the following analyses using data on probeset level, but it could also be done after summarizing the expression for all probe sets targeting the same gene. pca <- prcomp ( t ( exprs ( eset )), scale. = TRUE ) Plot the sample representation from the PCA. How much variance is encoded by each principal component? Are the principal components encoding known information? plot ( pca $ x , pch = 19 , cex = 2 ) plot ( pca ) pca $ sdev ^ 2 / sum ( pca $ sdev ^ 2 ) plot ( pca $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ disease.state )) legend ( \"topright\" , legend = levels ( factor ( pData ( eset ) $ disease.state )), col = 1 : 4 , pch = 19 ) It is common practice to filter out lowly varying genes before standardizing the data set and performing PCA. We will try this approach, using two different filter thresholds, leaving 5,000 and 100 variables, respectively. vars <- apply ( exprs ( eset ), 1 , var ) vars.order <- order ( vars , decreasing = TRUE ) pca.5000 <- prcomp ( t ( exprs ( eset )[ vars.order [ 1 : 5000 ], ]), scale. = TRUE ) plot ( pca.5000 $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ disease.state )) legend ( \"topright\" , legend = levels ( factor ( pData ( eset ) $ disease.state )), col = 1 : 4 , pch = 19 ) pca.100 <- prcomp ( t ( exprs ( eset )[ vars.order [ 1 : 100 ], ]), scale. = TRUE ) plot ( pca.100 $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ disease.state )) legend ( \"topright\" , legend = levels ( factor ( pData ( eset ) $ disease.state )), col = 1 : 4 , pch = 19 ) What do you notice in the last plot? The PC2 clearly splits the samples into two groups. Does this correspond to any known sample annotation? plot ( pca.100 $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ infection )) How can we find out what this is? One way is to look at the genes that are responsible for the split, that is, the ones that are contributing a lot to PC2. pc2.weights <- data.frame ( pca.100 $ rotation [, 2 , drop = FALSE ]) pc2.weights $ ChromosomeLoc <- fData ( eset )[ rownames ( pc2.weights ), \"Chromosome location\" ] head ( pc2.weights [ order ( pc2.weights $ PC2 ), ]) tail ( pc2.weights [ order ( pc2.weights $ PC2 ), ]) Now what do you think this component represents? Golub leukemia data In this exercise we study a microarray data set that is available as an expression set in R, from the library golubEsets. We load the data and extract the expression values and a sample annotation. library ( golubEsets ) data ( Golub_Train ) golub.expr = exprs ( Golub_Train ) golub.sample.annot = Golub_Train $ ALL.AML First, we apply PCA to the unstandardized data. pca.golub.cov = prcomp ( t ( golub.expr ), center = TRUE , scale. = FALSE ) plot ( pca.golub.cov $ x [, 1 : 2 ], col = golub.sample.annot , pch = 19 ) We also apply PCA to the standardized data. pca.golub.corr = prcomp ( t ( golub.expr ), center = TRUE , scale. = TRUE ) plot ( pca.golub.corr $ x [, 1 : 2 ], col = golub.sample.annot , pch = 19 ) Try also to plot the second and third principal components instead of the first two. What may be the reason for the discrepancy between the two results? For high-dimensional data sets like microarrays it is common to filter out the variables with low variance before applying the PCA. Extract the 1,000 variables with the highest variance from the Golub data set, and apply PCA to the standardized data sets to focus mainly on the correlations between the remaining variables. golub.expr.filtered = golub.expr [ order ( apply ( golub.expr , 1 , var ), decreasing = TRUE )[ 1 : 1000 ], ] pca.golub.filtered.corr = prcomp ( t ( golub.expr.filtered ), center = TRUE , scale. = TRUE ) plot ( pca.golub.filtered.corr $ x [, 1 : 2 ], col = golub.sample.annot , pch = 19 ) Compare the resulting plot to the previous two plots. What happens if you apply PCA to the unstandardized, filtered data set? Swiss banknote data In this exercise we consider a data set containing measurements from 100 genuine Swiss banknotes and 100 counterfeit notes. For each banknote there are six measurements; the length of the bill, the width of the left and right edges, the widths of the bottom and top margin, and the length of the image diagonal. The data set is available in the R package alr3. First, we load the data and look at the summary library ( alr3 ) data ( banknote ) summary ( banknote ) Then we apply PCA after scaling each variable to unit variance. pca.banknote = prcomp ( banknote [, 1 : 6 ], center = TRUE , scale. = TRUE ) summary ( pca.banknote ) In this example, we will create a biplot manually, by overlaying the sample and variable PCA plots plot ( pca.banknote $ x [, 1 : 2 ], col = c ( \"black\" , \"green\" )[ banknote [, 7 ] + 1 ], pch = 19 ) arrows ( 0 , 0 , 2 * pca.banknote $ rotation [, 1 ], 2 * pca.banknote $ rotation [, 2 ], col = \"red\" , angle = 20 , length = 0.1 ) text ( 2.4 * pca.banknote $ rotation [, 1 : 2 ], colnames ( banknote [, 1 : 6 ]), col = \"red\" ) In the resulting plot, the black and green points represent the genuine and counterfeit notes, respectively. The red arrows correspond to the contributions of the six variables to the principal components. In what ways did the counterfeiters fail to mimic the genuine notes? Look at the individual variables to verify. par ( mfrow = c ( 2 , 3 )) for ( v in c ( \"Length\" , \"Left\" , \"Right\" , \"Bottom\" , \"Top\" , \"Diagonal\" )) { boxplot ( banknote [, v ] ~ banknote [, \"Y\" ], xlab = \"Banknote status\" , ylab = v ) } Points in plates Import the data from dataClustering.csv What is the dimension of this dataset? How many data point do we have? Answer library ( \"cluster\" ) mydata1 <- read.csv ( \"dataClustering.csv\" ) df <- data.frame ( mydata1 $ Coord_X , mydata1 $ Coord_Y ) colnames ( df ) <- c ( \"X\" , \"Y\" ) plot ( df $ X , df $ Y , pch = 20 ) dim ( df ) Evaluate Euclidean distance of points in a plates Answer df.dist <- dist ( df ) Classify points to find clusters using hierarchical clustering and the average agglomeration method Answer df.h <- hclust ( df.dist , \"ave\" ) plot ( df.h ) colorScale <- colorRampPalette ( c ( \"blue\" , \"green\" , \"yellow\" , \"red\" , \"darkred\" ))( 1000 ) heatmap ( as.matrix ( df.dist ), Colv = NA , Rowv = NA , scale = \"none\" , col = colorScale ) We expect to have 3 clusters. When you apply k-means algorithm using 1 iteration, does it differ from applying it using 10 or 100 iterations? Answer kmeans ( df , 3 ) cl.1 <- kmeans ( df , 3 , iter.max = 1 ) plot ( df , col = cl.1 $ cluster ) points ( cl.1 $ centers , col = 1 : 5 , pch = 8 ) kmeans ( df , 3 ) cl.1 <- kmeans ( df , 3 , iter.max = 1 ) plot ( df , col = cl.1 $ cluster ) points ( cl.1 $ centers , col = 1 : 5 , pch = 8 ) cl.10 <- kmeans ( df , 3 , iter.max = 10 ) plot ( df , col = cl.10 $ cluster ) points ( cl.10 $ centers , col = 1 : 5 , pch = 8 ) cl.100 <- kmeans ( df , 3 , iter.max = 100 ) plot ( df , col = cl.100 $ cluster ) points ( cl.100 $ centers , col = 1 : 5 , pch = 8 ) What is the outcome of the C-means clustering? install.packages ( \"e1071\" ) library ( e1071 ) ? cmeans Answer cmeans ( df , 3 ) cl.1 <- cmeans ( df , 3 , iter.max = 1 ) plot ( df , col = cl.1 $ cluster ) points ( cl.1 $ centers , col = 1 : 5 , pch = 8 ) cl.10 <- cmeans ( df , 3 , iter.max = 10 ) plot ( df , col = cl.10 $ cluster ) points ( cl.10 $ centers , col = 1 : 5 , pch = 8 ) cl.100 <- cmeans ( df , 3 , iter.max = 100 ) plot ( df , col = cl.100 $ cluster ) points ( cl.100 $ centers , col = 1 : 5 , pch = 8 ) What are the top 3 models mclustBIC function suggests based on the BIC criterion? How many clusters did it find using the top model? Plot the outcome Answer library ( \"mclust\" ) BIC <- mclustBIC ( df ) plot ( BIC ) summary ( BIC ) mod1 <- Mclust ( df , x = BIC ) summary ( mod1 , parameters = TRUE ) plot ( mod1 , what = \"classification\" ) mod2 <- Mclust ( df , modelName = c ( \"VEE\" )) plot ( mod2 , what = \"classification\" ) mod3 <- Mclust ( df , modelName = \"EEE\" , G = 9 ) plot ( mod3 , what = \"classification\" )","title":"Day 4"},{"location":"day4/#clustering-and-pca","text":"In this section, you will find the R code that we will use during the course. We will explain the code and output during correction of the exercises. Slides of lectures: Download slides","title":"Clustering and PCA"},{"location":"day4/#the-iris-data-set","text":"We will study the iris data set, containing measurements of the petal length and width and the sepal length and width for 150 iris flowers of three different types. The data set is available within R. First, we load the data, display its structure and summarize its content. data ( iris ) str ( iris ) summary ( iris ) head ( iris ) Are the range of values for the individual variables comparable? What is the dimensionality of the data (the number of coordinates used to represent the samples)? Next, we create univariate plots to visualize the variables individually or pairwise. boxplot ( iris [, 1 : 4 ]) pairs ( iris [, 1 : 4 ], col = iris $ Species , pch = 19 ) What do the colors represent? Are there any apparent associations between the variables? We can also create boxplots of the individual variables stratified by iris type. boxplot ( iris $ Petal.Width ~ iris $ Species ) Now, we apply PCA to the data, using the prcomp function. Make sure that you know what the arguments to the function stand for. pca.iris.cov = prcomp ( iris [, 1 : 4 ], center = TRUE , scale. = FALSE ) plot ( pca.iris.cov $ x , col = iris $ Species , pch = 19 ) What can you see in the sample plot? In addition to the sample configuration, we also look at the variable loadings, to see which variables contribute most to the principal components, and at the fraction of variance that is explained by each principal component. pca.iris.cov $ rotation summary ( pca.iris.cov ) Which variables have the strongest influence on each of the first two principal components? How can you use this information to interpret the sample PCA representation? We can now visualize the sample scores and the variable loadings together in a biplot. biplot ( pca.iris.cov , scale = 0 ) What do the different objects in the biplot represent? How are they connected to the output from prcomp? The sample scores from PCA are obtained as linear combinations of the four measured variables, with weights given by the variable loadings. Verify that this is the case e.g. for the scores for the first flower. iris.centered = scale ( iris [, 1 : 4 ], center = TRUE , scale = FALSE ) # compute linear combination scores.sample1 = iris.centered[1, ] %*% pca.iris.cov$rotation # compare to PCA scores pca.iris.cov $ x [ 1 , ] / scores.sample1 The PCA just applied was based on the non-standardized variables, that is, on the covariance matrix. Where was this specified? Can we gain some understanding of the results (the variable loadings) by studying the individual variable variances? var ( iris [, 1 : 4 ]) Next, we rerun the analysis with standardized variables. pca.iris.corr = prcomp ( iris [, 1 : 4 ], center = TRUE , scale. = TRUE ) plot ( pca.iris.corr $ x , col = iris $ Species , pch = 19 ) pca.iris.corr $ rotation summary ( pca.iris.corr ) biplot ( pca.iris.corr , scale = 0 ) How did the results change? Can we understand the change by using the information form the pairwise scatterplots and the individual variances? Which variables appear to be most important for separating the two groups of iris flowers seen in the PCA plot? To see the amount of variance captured by each of the principal components, construct scree plots for the two PCAs. par ( mfrow = c ( 1 , 2 )) screeplot ( pca.iris.cov , type = \"line\" ) screeplot ( pca.iris.corr , type = \"line\" ) How many principal components are needed to explain most of the variance in the data?","title":"The Iris data set"},{"location":"day4/#dengue-fever-example","text":"In this example, we will use a gene expression data set aimed at studying the differences between patients with dengue fever and healthy controls or convalescents. The data set corresponds to a publication by Kwissa et al (Cell Host & Microbe 16:115-127 (2014)), and the expression matrix has been deposited in Gene Expression Omnibus (GEO) with accession number GDS5093.","title":"Dengue fever example"},{"location":"day4/#data-retrieval","text":"We will use the GEOquery package to retrieve the data and platform annotation information. The GEOquery package is available from Bioconductor, and can be installed in the following way: source ( \"https://bioconductor.org/biocLite.R\" ) biocLite ( \"GEOquery\" ) And the data retrieved like this and then have a look at the downloaded data : library ( GEOquery ) ## Retrieve the data from GEO gds <- getGEO ( \"GDS5093\" ) ## If you have already downloaded the data, you can load the soft file directly ## gds <- getGEO(\"GDS5093.soft.gz\") ## Look at the elements of the downloaded data head(Columns(gds)) head ( Table ( gds )) head ( Meta ( gds )) ## Convert the gds object to an expression set eset <- GDS2eSet ( gds ) We have converted the downloaded object to a so called expression set. The expression set is a special Bioconductor data container that is commonly used to store all information associated with a microarray experiment. Using different accessor functions, we can extract the different types of information stored in the expression set (expression values, sample annotations, variable annotations) ## Sample information head ( pData ( eset )) ## Expression matrix head ( exprs ( eset )) ## Feature information head ( fData ( eset ))","title":"Data retrieval"},{"location":"day4/#principal-component-analysis","text":"First we perform a PCA using all the variables in the data set. We will perform the following analyses using data on probeset level, but it could also be done after summarizing the expression for all probe sets targeting the same gene. pca <- prcomp ( t ( exprs ( eset )), scale. = TRUE ) Plot the sample representation from the PCA. How much variance is encoded by each principal component? Are the principal components encoding known information? plot ( pca $ x , pch = 19 , cex = 2 ) plot ( pca ) pca $ sdev ^ 2 / sum ( pca $ sdev ^ 2 ) plot ( pca $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ disease.state )) legend ( \"topright\" , legend = levels ( factor ( pData ( eset ) $ disease.state )), col = 1 : 4 , pch = 19 ) It is common practice to filter out lowly varying genes before standardizing the data set and performing PCA. We will try this approach, using two different filter thresholds, leaving 5,000 and 100 variables, respectively. vars <- apply ( exprs ( eset ), 1 , var ) vars.order <- order ( vars , decreasing = TRUE ) pca.5000 <- prcomp ( t ( exprs ( eset )[ vars.order [ 1 : 5000 ], ]), scale. = TRUE ) plot ( pca.5000 $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ disease.state )) legend ( \"topright\" , legend = levels ( factor ( pData ( eset ) $ disease.state )), col = 1 : 4 , pch = 19 ) pca.100 <- prcomp ( t ( exprs ( eset )[ vars.order [ 1 : 100 ], ]), scale. = TRUE ) plot ( pca.100 $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ disease.state )) legend ( \"topright\" , legend = levels ( factor ( pData ( eset ) $ disease.state )), col = 1 : 4 , pch = 19 ) What do you notice in the last plot? The PC2 clearly splits the samples into two groups. Does this correspond to any known sample annotation? plot ( pca.100 $ x , pch = 19 , cex = 2 , col = factor ( pData ( eset ) $ infection )) How can we find out what this is? One way is to look at the genes that are responsible for the split, that is, the ones that are contributing a lot to PC2. pc2.weights <- data.frame ( pca.100 $ rotation [, 2 , drop = FALSE ]) pc2.weights $ ChromosomeLoc <- fData ( eset )[ rownames ( pc2.weights ), \"Chromosome location\" ] head ( pc2.weights [ order ( pc2.weights $ PC2 ), ]) tail ( pc2.weights [ order ( pc2.weights $ PC2 ), ]) Now what do you think this component represents?","title":"Principal component analysis"},{"location":"day4/#golub-leukemia-data","text":"In this exercise we study a microarray data set that is available as an expression set in R, from the library golubEsets. We load the data and extract the expression values and a sample annotation. library ( golubEsets ) data ( Golub_Train ) golub.expr = exprs ( Golub_Train ) golub.sample.annot = Golub_Train $ ALL.AML First, we apply PCA to the unstandardized data. pca.golub.cov = prcomp ( t ( golub.expr ), center = TRUE , scale. = FALSE ) plot ( pca.golub.cov $ x [, 1 : 2 ], col = golub.sample.annot , pch = 19 ) We also apply PCA to the standardized data. pca.golub.corr = prcomp ( t ( golub.expr ), center = TRUE , scale. = TRUE ) plot ( pca.golub.corr $ x [, 1 : 2 ], col = golub.sample.annot , pch = 19 ) Try also to plot the second and third principal components instead of the first two. What may be the reason for the discrepancy between the two results? For high-dimensional data sets like microarrays it is common to filter out the variables with low variance before applying the PCA. Extract the 1,000 variables with the highest variance from the Golub data set, and apply PCA to the standardized data sets to focus mainly on the correlations between the remaining variables. golub.expr.filtered = golub.expr [ order ( apply ( golub.expr , 1 , var ), decreasing = TRUE )[ 1 : 1000 ], ] pca.golub.filtered.corr = prcomp ( t ( golub.expr.filtered ), center = TRUE , scale. = TRUE ) plot ( pca.golub.filtered.corr $ x [, 1 : 2 ], col = golub.sample.annot , pch = 19 ) Compare the resulting plot to the previous two plots. What happens if you apply PCA to the unstandardized, filtered data set?","title":"Golub leukemia data"},{"location":"day4/#swiss-banknote-data","text":"In this exercise we consider a data set containing measurements from 100 genuine Swiss banknotes and 100 counterfeit notes. For each banknote there are six measurements; the length of the bill, the width of the left and right edges, the widths of the bottom and top margin, and the length of the image diagonal. The data set is available in the R package alr3. First, we load the data and look at the summary library ( alr3 ) data ( banknote ) summary ( banknote ) Then we apply PCA after scaling each variable to unit variance. pca.banknote = prcomp ( banknote [, 1 : 6 ], center = TRUE , scale. = TRUE ) summary ( pca.banknote ) In this example, we will create a biplot manually, by overlaying the sample and variable PCA plots plot ( pca.banknote $ x [, 1 : 2 ], col = c ( \"black\" , \"green\" )[ banknote [, 7 ] + 1 ], pch = 19 ) arrows ( 0 , 0 , 2 * pca.banknote $ rotation [, 1 ], 2 * pca.banknote $ rotation [, 2 ], col = \"red\" , angle = 20 , length = 0.1 ) text ( 2.4 * pca.banknote $ rotation [, 1 : 2 ], colnames ( banknote [, 1 : 6 ]), col = \"red\" ) In the resulting plot, the black and green points represent the genuine and counterfeit notes, respectively. The red arrows correspond to the contributions of the six variables to the principal components. In what ways did the counterfeiters fail to mimic the genuine notes? Look at the individual variables to verify. par ( mfrow = c ( 2 , 3 )) for ( v in c ( \"Length\" , \"Left\" , \"Right\" , \"Bottom\" , \"Top\" , \"Diagonal\" )) { boxplot ( banknote [, v ] ~ banknote [, \"Y\" ], xlab = \"Banknote status\" , ylab = v ) }","title":"Swiss banknote data"},{"location":"day4/#points-in-plates","text":"Import the data from dataClustering.csv What is the dimension of this dataset? How many data point do we have? Answer library ( \"cluster\" ) mydata1 <- read.csv ( \"dataClustering.csv\" ) df <- data.frame ( mydata1 $ Coord_X , mydata1 $ Coord_Y ) colnames ( df ) <- c ( \"X\" , \"Y\" ) plot ( df $ X , df $ Y , pch = 20 ) dim ( df ) Evaluate Euclidean distance of points in a plates Answer df.dist <- dist ( df ) Classify points to find clusters using hierarchical clustering and the average agglomeration method Answer df.h <- hclust ( df.dist , \"ave\" ) plot ( df.h ) colorScale <- colorRampPalette ( c ( \"blue\" , \"green\" , \"yellow\" , \"red\" , \"darkred\" ))( 1000 ) heatmap ( as.matrix ( df.dist ), Colv = NA , Rowv = NA , scale = \"none\" , col = colorScale ) We expect to have 3 clusters. When you apply k-means algorithm using 1 iteration, does it differ from applying it using 10 or 100 iterations? Answer kmeans ( df , 3 ) cl.1 <- kmeans ( df , 3 , iter.max = 1 ) plot ( df , col = cl.1 $ cluster ) points ( cl.1 $ centers , col = 1 : 5 , pch = 8 ) kmeans ( df , 3 ) cl.1 <- kmeans ( df , 3 , iter.max = 1 ) plot ( df , col = cl.1 $ cluster ) points ( cl.1 $ centers , col = 1 : 5 , pch = 8 ) cl.10 <- kmeans ( df , 3 , iter.max = 10 ) plot ( df , col = cl.10 $ cluster ) points ( cl.10 $ centers , col = 1 : 5 , pch = 8 ) cl.100 <- kmeans ( df , 3 , iter.max = 100 ) plot ( df , col = cl.100 $ cluster ) points ( cl.100 $ centers , col = 1 : 5 , pch = 8 ) What is the outcome of the C-means clustering? install.packages ( \"e1071\" ) library ( e1071 ) ? cmeans Answer cmeans ( df , 3 ) cl.1 <- cmeans ( df , 3 , iter.max = 1 ) plot ( df , col = cl.1 $ cluster ) points ( cl.1 $ centers , col = 1 : 5 , pch = 8 ) cl.10 <- cmeans ( df , 3 , iter.max = 10 ) plot ( df , col = cl.10 $ cluster ) points ( cl.10 $ centers , col = 1 : 5 , pch = 8 ) cl.100 <- cmeans ( df , 3 , iter.max = 100 ) plot ( df , col = cl.100 $ cluster ) points ( cl.100 $ centers , col = 1 : 5 , pch = 8 ) What are the top 3 models mclustBIC function suggests based on the BIC criterion? How many clusters did it find using the top model? Plot the outcome Answer library ( \"mclust\" ) BIC <- mclustBIC ( df ) plot ( BIC ) summary ( BIC ) mod1 <- Mclust ( df , x = BIC ) summary ( mod1 , parameters = TRUE ) plot ( mod1 , what = \"classification\" ) mod2 <- Mclust ( df , modelName = c ( \"VEE\" )) plot ( mod2 , what = \"classification\" ) mod3 <- Mclust ( df , modelName = \"EEE\" , G = 9 ) plot ( mod3 , what = \"classification\" )","title":"Points in plates"},{"location":"exam/","text":"EXAM The participants who need credits must answer the following questions and send the results as an R script with comments to rachel.marcone@sib.swiss until latest February 2023. Data: A set of data collected by Heinz et al.(* Heinz G, Peterson LJ, Johnson RW, Kerk CJ Journal of Statistics Education Volume 11, Number 2 (2003) jse.amstat.org/v11n2/datasets.heinz.html, by Grete Heinz, Louis J. Peterson, Roger W. Johnson, and Carter J. Kerk, all rights reserved) is available in the file IS_23_exam.csv Goals: Get to know the overall structure of the data. Summarize variables numerically and graphically. Model relationships between variables. Download exercise material Observations Have look at the file in a text editor to get familiar with it Open a new script file in R studio, comment it and save it. Read the file, assign it to object \u201cIS_23_exam\u201d. Examine \u201cIS_23_exam\u201d. a) How many observations and variables does the dataset have ? b) What are the names and types of the variables ? c) Get the summary statistics of \u201cIS_23_exam\u201d. Make a scatter plot of all pairs of variables in the dataset. Calculate the BMI of each person and add it as an extra variable \u201cbmi\u201d to your dataframe (Google the BMI formula). Modelling Is there a significant difference in bmi means between males and females? How strong is the linear (Pearson) correlation between chest girth and height? Is it significant? If you model a linear relationship, how much does the chest girth increase per added cm of height? Is the change significant? What if you do this for males and females separately? Come up with a question for hypothesis testing of your own that includes one or more variable(s) of your choosing from the data set. Make plots as seen in the course to try to give visualization\u2010based answers to this question. Test your hypothesis using the tests and modeling techniques from the course, based on the type of variables you have. Include tests of the assumptions where appropriate. PCA and clustering Perform a PCA using all the variables in the dataset, discarding the age and gender Do a PCA plot, using different colors for the data points for males and females. How much variance is encoded by each principal component ? Which variables have the strongest influence on each of the first two principal components ? Create a new dataframe called PCA_coord with the coordinates of the data points on PC1 and PC2 Evaluate the Euclidean distance between the data points Generate a heatmap of the distance matrix Identify clusters of the data points using a method of your choice, that has been shown during the course","title":"Exam"},{"location":"exam/#exam","text":"The participants who need credits must answer the following questions and send the results as an R script with comments to rachel.marcone@sib.swiss until latest February 2023. Data: A set of data collected by Heinz et al.(* Heinz G, Peterson LJ, Johnson RW, Kerk CJ Journal of Statistics Education Volume 11, Number 2 (2003) jse.amstat.org/v11n2/datasets.heinz.html, by Grete Heinz, Louis J. Peterson, Roger W. Johnson, and Carter J. Kerk, all rights reserved) is available in the file IS_23_exam.csv Goals: Get to know the overall structure of the data. Summarize variables numerically and graphically. Model relationships between variables. Download exercise material","title":"EXAM"},{"location":"exam/#observations","text":"Have look at the file in a text editor to get familiar with it Open a new script file in R studio, comment it and save it. Read the file, assign it to object \u201cIS_23_exam\u201d. Examine \u201cIS_23_exam\u201d. a) How many observations and variables does the dataset have ? b) What are the names and types of the variables ? c) Get the summary statistics of \u201cIS_23_exam\u201d. Make a scatter plot of all pairs of variables in the dataset. Calculate the BMI of each person and add it as an extra variable \u201cbmi\u201d to your dataframe (Google the BMI formula).","title":"Observations"},{"location":"exam/#modelling","text":"Is there a significant difference in bmi means between males and females? How strong is the linear (Pearson) correlation between chest girth and height? Is it significant? If you model a linear relationship, how much does the chest girth increase per added cm of height? Is the change significant? What if you do this for males and females separately? Come up with a question for hypothesis testing of your own that includes one or more variable(s) of your choosing from the data set. Make plots as seen in the course to try to give visualization\u2010based answers to this question. Test your hypothesis using the tests and modeling techniques from the course, based on the type of variables you have. Include tests of the assumptions where appropriate.","title":"Modelling"},{"location":"exam/#pca-and-clustering","text":"Perform a PCA using all the variables in the dataset, discarding the age and gender Do a PCA plot, using different colors for the data points for males and females. How much variance is encoded by each principal component ? Which variables have the strongest influence on each of the first two principal components ? Create a new dataframe called PCA_coord with the coordinates of the data points on PC1 and PC2 Evaluate the Euclidean distance between the data points Generate a heatmap of the distance matrix Identify clusters of the data points using a method of your choice, that has been shown during the course","title":"PCA and clustering"},{"location":"links/","text":"Links that are useful https://www.statlearning.com/, a book to be downloaded to learn statistics and apply in R. https://towardsdatascience.com/maximum-likelihood-ml-vs-reml-78cf79bef2cf, a mathematical explanation showing the difference between ML and REML with exact formulas. https://topepo.github.io/caret/, package for cross-validation or leave-one-out GLMnet and penalized are packages for penalization and regularization : https://glmnet.stanford.edu/articles/glmnet.html and https://cran.r-project.org/web/packages/penalized/index.html","title":"Useful links"},{"location":"links/#links-that-are-useful","text":"https://www.statlearning.com/, a book to be downloaded to learn statistics and apply in R. https://towardsdatascience.com/maximum-likelihood-ml-vs-reml-78cf79bef2cf, a mathematical explanation showing the difference between ML and REML with exact formulas. https://topepo.github.io/caret/, package for cross-validation or leave-one-out GLMnet and penalized are packages for penalization and regularization : https://glmnet.stanford.edu/articles/glmnet.html and https://cran.r-project.org/web/packages/penalized/index.html","title":"Links that are useful"},{"location":"precourse/","text":"Previous knowledge Knowledge: No prior statistical knowledge is required in order to attend the course Participants do not need any experience in R before the course Technical To do the exercises, you are required to have your own computer with at least 4 Gb of RAM and with an internet connection, as well as the latest the version of R and the free version of RStudio installed.","title":"Precourse preparation"},{"location":"precourse/#previous-knowledge","text":"","title":"Previous knowledge"},{"location":"precourse/#knowledge","text":"No prior statistical knowledge is required in order to attend the course Participants do not need any experience in R before the course","title":"Knowledge:"},{"location":"precourse/#technical","text":"To do the exercises, you are required to have your own computer with at least 4 Gb of RAM and with an internet connection, as well as the latest the version of R and the free version of RStudio installed.","title":"Technical"}]}